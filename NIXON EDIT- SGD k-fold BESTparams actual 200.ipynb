{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To note: Running this on different systems (i.e. local, SCW, server) will result in slight changes needing to the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x7fc253d52970>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from PIL import ImageEnhance\n",
    "import matplotlib.pyplot as plt\n",
    "import imagesize\n",
    "import subprocess\n",
    "sys.path\n",
    "from IPython.core.debugger import set_trace\n",
    "import scipy.ndimage\n",
    "import matplotlib.patches as patches\n",
    "# plt.rcParams['figure.figsize'] = [12,12]\n",
    "# sys.path.append('/workspace/myFile/Mask_RCNN_Tutorial/')\n",
    "from tqdm import tqdm\n",
    "from torch import nn as nn\n",
    "from torch import optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import re\n",
    "import time\n",
    "import copy\n",
    "import pylab\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tempfile import TemporaryDirectory\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import glob\n",
    "# Rather than have a messy notebook with a load of functions we can store them in separate .py files and import them\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "writer = SummaryWriter(\"Experiments/200againbest_model_params\")    # This determines the tensorboard file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNADataset(object):\n",
    "    def __init__(self, root, transforms, labels, imDx = False):\n",
    "        self.root, self.transforms, self.labels = root, transforms, labels\n",
    "    \n",
    "        # load all image files, sorting them to ensure they are aligned\n",
    "        self.imgDir = glob.glob(root+\"*/*.tiff\")\n",
    "        Damagednuclei= [x for x in self.imgDir if 'Damaged_nuclei_' in x]\n",
    "        Undamagednuclei= [x for x in self.imgDir if \"No_damage_nuclei\" in x]\n",
    "        #np.random.shuffle(Undamagednuclei)\n",
    "        #Undamagednuclei=Undamagednuclei[:10000]\n",
    "        self.imgDir= Damagednuclei+Undamagednuclei\n",
    "        size80=[]\n",
    "        for x in self.imgDir:\n",
    "            img = Image.open(x) # Open image\n",
    "            w,h=img.size\n",
    "            if w<=80 and h<=80:\n",
    "                size80.append(x)\n",
    "        self.imgDir=size80              \n",
    "        \n",
    "        self.imgs = sorted(self.imgDir) # list of images\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.imgs[idx]\n",
    "       \n",
    "        # Transform images into tensors\n",
    "        img = Image.open(img_path) # Open image\n",
    "        w,h=img.size\n",
    "        img = np.array(img) # Convert image into an array\n",
    "        img = np.float32(np.divide(img, 2**16)) # Ensure all values are floats\n",
    "        \n",
    "        result=np.zeros((80,80), dtype=np.float32)\n",
    "        x_center = (80 - w) // 2\n",
    "        y_center = (80 - h) // 2 # copy img image into center of result image\n",
    "        result[y_center:y_center+h, x_center:x_center+w] = img\n",
    "        img = result\n",
    "        \n",
    "        targetlab=\"\"\n",
    "        if img_path.find('No_damage_nuclei') != -1:\n",
    "            targetlab= 'Undamaged'\n",
    "        if img_path.find('Damaged_nuclei_') != -1:\n",
    "            targetlab= 'Damaged'  # Find labels corresponding to image\n",
    "        target = self.labels.index(targetlab) # Get the label and assign to a value\n",
    "        \n",
    "        # Convert label to tensor\n",
    "        #torch.to\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "#             #print('In the transforms')\n",
    "        imNo = idx\n",
    "        return img, target, imNo\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=\"/workspace/myFile/Output/17052023/\"\n",
    "p= glob.glob(root+\"*/*.tiff\")\n",
    "Damagednuclei= [x for x in p if 'Damaged_nuclei_' in x]\n",
    "Undamagednuclei= [x for x in p if \"No_damage_nuclei\" in x]\n",
    "#np.random.shuffle(Undamagednuclei)\n",
    "#Undamagednuclei=Undamagednuclei[:10000]\n",
    "#Undamagednuclei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imDr = \"/workspace/myFile/Output/17052023/\"  # Image patches directory\n",
    "\n",
    "labels = ['Damaged', 'Undamaged']  # Your labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data augmentation\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "\n",
    "    transforms.append(T.ToTensor())\n",
    "    #transforms.append(T.Normalize([0.0019368887995516483], [0.00672996630111016]))\n",
    "    #transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    \n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(p=0.5))\n",
    "        transforms.append(T.RandomVerticalFlip(p=0.5))\n",
    "    \n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, train, test, num_epochs=25):\n",
    "    since = time.time()\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "            dataset_train = torch.utils.data.Subset(dataSetTrain, train[epoch])\n",
    "            dataset_test = torch.utils.data.Subset(dataSetTest, test[epoch])\n",
    "            data_loader_train = torch.utils.data.DataLoader(\n",
    "                dataset_train, batch_size=128, shuffle=True, num_workers=0)\n",
    "\n",
    "            data_loader_test = torch.utils.data.DataLoader(\n",
    "                dataset_test, batch_size=128, shuffle=False, num_workers=0)\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                    dataloaders = data_loader_train\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "                    dataloaders = data_loader_test\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "                damaged_len=0\n",
    "                damaged_corrects = 0\n",
    "                undamaged_len=0\n",
    "                undamaged_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels, imNo in dataloaders:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)    # Loss\n",
    "                    nada=torch.tensor(np.zeros(len(labels))).to(device)\n",
    "                    uno=nada+1\n",
    "                    falseneg=preds+1\n",
    "                    falsepos=preds-1\n",
    "                    FPplusTN=torch.sum(labels==nada)\n",
    "                    TPplusFN=torch.sum(labels==uno)\n",
    "                    FN=torch.sum(labels==falseneg)\n",
    "                    FP=torch.sum(labels==falsepos)\n",
    "                    TN=FPplusTN-FP\n",
    "                    TP=TPplusFN-FN\n",
    "                    running_corrects += torch.sum(preds == labels.data) # Accuracy\n",
    "                    damaged_len+=TPplusFN\n",
    "                    damaged_corrects += TP\n",
    "                    undamaged_len+=FPplusTN\n",
    "                    undamaged_corrects += TN\n",
    "                damaged_acc = damaged_corrects / damaged_len\n",
    "                undamaged_acc = undamaged_corrects/undamaged_len\n",
    "                epoch_acc= (damaged_acc+undamaged_acc)/2\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]    # Loss metric per epoch\n",
    "                #epoch_acc = running_corrects.double() / dataset_sizes[phase]    # Accuracy metric per epoch\n",
    "\n",
    "                if phase == \"train\":    # This is the tensorboard code that writes accuracy and loss metrics\n",
    "                    writer.add_scalar(\"Train/Accuracy\", epoch_acc, epoch)\n",
    "                    writer.add_scalar(\"Train/Loss\", epoch_loss, epoch)\n",
    "                else:\n",
    "                    writer.add_scalar(\"Validation/Accuracy\", epoch_acc, epoch)\n",
    "                    writer.add_scalar(\"Validation/Loss\", epoch_loss, epoch)\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and epoch_acc >= best_acc: \n",
    "                    # This compares validation accuracy to previous bests and adjusts model weights accordingly\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "                    con_mat= torch.tensor([[damaged_corrects, damaged_len-damaged_corrects],[undamaged_len-undamaged_corrects,undamaged_corrects]])\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since  # Nice way to measure training time but info also stored (indirectly) by tensorboard\n",
    "        print(\n",
    "            f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "        #labels= labels.cpu().numpy()\n",
    "        #preds=preds.cpu().numpy()\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    writer.close()\n",
    "    return model, con_mat, best_acc   # We want to return the model because its the model, also confusion matrix for later analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is your RESNET\n",
    "# Initialize CNN with kaiming\n",
    "def init_cnn(m):\n",
    "    # Set the weights of the RESNET\n",
    "    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)\n",
    "    if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight)\n",
    "    for l in m.children(): init_cnn(l)\n",
    "\n",
    "\n",
    "# noop function for returning nothing\n",
    "def noop(x): return x\n",
    "# activation function(RELU)\n",
    "act_fn = nn.ReLU(inplace=True)\n",
    "\n",
    "# Flatten\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): return x.view(x.size(0), -1)\n",
    "\n",
    "# Make a convolution\n",
    "def conv(ni, nf, ks=3, stride=1, bias=False):\n",
    "    return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)\n",
    "\n",
    "# Create a convuolutional layer with convolution and batch norm\n",
    "def conv_layer(ni, nf, ks=3, stride=1, zero_bn=False, act=True):\n",
    "    bn = nn.BatchNorm2d(nf) # get a 2d batch norm from Pytorhc\n",
    "    nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n",
    "    layers = [conv(ni, nf, ks, stride=stride), bn]\n",
    "    if act: layers.append(act_fn) # add in the activation function if act is true\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "# Resblock\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, expansion, ni, nh, stride = 1):\n",
    "        super().__init__()\n",
    "        # ni - number of inputs channels, nf - number of filters\n",
    "        # nh - number of filters in first conv\n",
    "        # expansion is 1 for resnet 18, 34 and 4 for larger networks\n",
    "        nf, ni = nh*expansion, ni*expansion\n",
    "        layers = [conv_layer(ni, nh, 3, stride = stride), # for resnet < 34 2 convs per resblock\n",
    "                 conv_layer(nh, nf, 3, zero_bn = True, act = False)\n",
    "                 ] if expansion == 1 else [ # for RESNET > 34 then 3 convs per block with bottleneck\n",
    "                            conv_layer(ni, nh, 1),\n",
    "                            conv_layer(nh, nh, 3, stride = stride),\n",
    "                            conv_layer(nh, nf, 1, zero_bn = True, act = False)\n",
    "        ]\n",
    "        self.convs = nn.Sequential(*layers) # Creates the conv layers\n",
    "        self.idconv = noop if ni==nf else conv_layer(ni, nf, 1, act = False) # id convolution ()\n",
    "        self.pool = noop if stride== 1 else nn.AvgPool2d(2, ceil_mode = True) # average pool on \n",
    "        \n",
    "    def forward(self, x): \n",
    "        # Forward function adds the convolution part to the id part \n",
    "        #return act_fn(self.convs(x)) + self.idconv(self.pool(x))\n",
    "        return act_fn(self.convs(x) + self.idconv(self.pool(x)))\n",
    "\n",
    "# XResnet\n",
    "class XResNet(nn.Sequential):\n",
    "    @classmethod\n",
    "    def create(cls, expansion, layers, c_in=3, c_out=1000):\n",
    "        nfs = [c_in, (c_in + 1)*8, 64, 64] # number of filters in stem layer (c_in is number of image channels)\n",
    "        stem = [conv_layer(nfs[i], nfs[i+1], stride=2 if i==0 else 1)\n",
    "            for i in range(3)]\n",
    "\n",
    "        nfs = [64//expansion,64,128,256,512]\n",
    "        res_layers = [cls._make_layer(expansion, nfs[i], nfs[i+1],\n",
    "                                      n_blocks=l, stride=1 if i==0 else 2)\n",
    "                  for i,l in enumerate(layers)]\n",
    "        res = cls(\n",
    "        *stem,\n",
    "        nn.MaxPool2d(kernel_size=3, stride = 2, padding = 1), # then a max pooling layer\n",
    "        *res_layers,\n",
    "        nn.AdaptiveAvgPool2d(1), Flatten(), \n",
    "        nn.Linear(nfs[-1]*expansion, c_out)\n",
    "        )\n",
    "        init_cnn(res)\n",
    "        return res\n",
    "        \n",
    "    @staticmethod\n",
    "    def _make_layer(expansion, ni, nf, n_blocks, stride): # returns a resblock\n",
    "        return nn.Sequential(\n",
    "        *[ResBlock(expansion, ni if i==0 else nf, nf, stride if i==0 else 1)\n",
    "         for i in range(n_blocks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xresnet18 (**kwargs): return XResNet.create(1, [2, 2,  2, 2], **kwargs)\n",
    "def xresnet34 (**kwargs): return XResNet.create(1, [3, 4,  6, 3], **kwargs)\n",
    "def xresnet50 (**kwargs): return XResNet.create(4, [3, 4,  6, 3], **kwargs)\n",
    "model18 = xresnet18(c_in = 1, c_out = 2)\n",
    "model34 = xresnet34(c_in = 1, c_out = 2)\n",
    "model50 = xresnet50(c_in = 1, c_out = 2)\n",
    "\n",
    "# Label smoothing cross entropy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def reduce_loss(loss, reduction='mean'):\n",
    "    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, ε:float=0.1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.ε,self.reduction = ε,reduction\n",
    "    \n",
    "    def forward(self, output, target):\n",
    "        c = output.size()[-1]\n",
    "        log_preds = F.log_softmax(output, dim=-1)\n",
    "        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n",
    "        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n",
    "        return lin_comb(loss/c, nll, self.ε)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions that shows the image, true class, predicted class and degree of prediction\n",
    "\n",
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    '''\n",
    "    output = net(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            labels[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            labels[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig\n",
    "\n",
    "def matplotlib_imshow(img, one_channel=True):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_calc(data_loader_test, classes, model_ft):       \n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for inputs, labels, imNo in data_loader_test:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                output = model_ft(inputs) # Feed Network\n",
    "\n",
    "                output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "                y_pred.extend(output) # Save Prediction\n",
    "                \n",
    "                labels = labels.data.cpu().numpy()\n",
    "                y_true.extend(labels) # Save Truth\n",
    "\n",
    "        # constant for classes\n",
    "        # classes = ('Alive', 'Dead')\n",
    "\n",
    "        # Build confusion matrix\n",
    "        cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "        return cf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5 #NUMBER OF K FOLDS\n",
    "epoch=range(200) #NUMBER OF EPOCHS\n",
    "\n",
    "torch.manual_seed(10)\n",
    "imIdx = torch.randperm(1).tolist()\n",
    "\n",
    "## Create dataset\n",
    "dataSetTrain = DNADataset(imDr, get_transform(train = True), labels, imDx=imIdx)\n",
    "dataSetTest = DNADataset(imDr, get_transform(train = False), labels, imDx=imIdx)\n",
    "\n",
    "data_loader_train=[]\n",
    "data_loader_test=[]\n",
    "\n",
    "# ## Create dataloaders\n",
    "# Get subset\n",
    "torch.manual_seed(10)\n",
    "TotalSetTrain=list(range(len(dataSetTrain)))\n",
    "np.random.shuffle(TotalSetTrain)\n",
    "TotalSetTest=list(range(len(dataSetTest)))\n",
    "np.random.shuffle(TotalSetTest)\n",
    "\n",
    "train=[]\n",
    "test=[]\n",
    "for epoch in epoch:\n",
    "    \n",
    "    Kfifth = len(dataSetTrain)//k\n",
    "    split= epoch%k\n",
    "    train.append(TotalSetTrain[(split+1)*Kfifth:]+TotalSetTrain[:Kfifth*split])\n",
    "    test.append(TotalSetTest[split*Kfifth:(split+1)*Kfifth])\n",
    "\n",
    "#dataset_train = torch.utils.data.Subset(dataSetTrain, indices[-noTrain:])\n",
    "\n",
    "#dataset_test = torch.utils.data.Subset(dataSetTest, indices[:-noTrain])\n",
    "#len(indices), len(indices[:-50]), len(indices[-50:]), 50/191, type(dataset_test)\n",
    "\n",
    "dataset_sizes = {'train': len(train[0]), 'val': len(test[0])}\n",
    "\n",
    "#dataset_train = torch.utils.data.Subset(dataSetTrain, train)\n",
    "#data_loader_train.append(dataset_train)\n",
    "#dataset_testo = torch.utils.data.Subset(dataSetTest, test)\n",
    "#data_loader_test.append(dataset_testo)\n",
    "# define training and validation data loaders\n",
    "\n",
    "\n",
    "# Collate function (gathers together the outputs)\n",
    "# def collate_fn(batch):\n",
    "#     return tuple(zip(*batch))\n",
    "\n",
    "#len(indices[-noTrain:]), dataset_sizes\n",
    "#dataset_test[0][1], dataset_test[3][1], dataset_test[-1][1], dataSetTrain.imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 0.5546 Acc: 0.7504\n",
      "val Loss: 0.4628 Acc: 0.7848\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 0.4464 Acc: 0.7990\n",
      "val Loss: 1.6271 Acc: 0.5026\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 0.4286 Acc: 0.8070\n",
      "val Loss: 0.8769 Acc: 0.6248\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 0.4115 Acc: 0.8180\n",
      "val Loss: 1.9555 Acc: 0.5134\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 0.4005 Acc: 0.8271\n",
      "val Loss: 0.4011 Acc: 0.8243\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 0.4041 Acc: 0.8218\n",
      "val Loss: 1.1320 Acc: 0.5496\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 0.3934 Acc: 0.8255\n",
      "val Loss: 0.3933 Acc: 0.8263\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 0.3779 Acc: 0.8372\n",
      "val Loss: 0.4234 Acc: 0.8074\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 0.3714 Acc: 0.8407\n",
      "val Loss: 0.3762 Acc: 0.8417\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 0.3657 Acc: 0.8431\n",
      "val Loss: 0.3638 Acc: 0.8428\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 0.3666 Acc: 0.8409\n",
      "val Loss: 0.6621 Acc: 0.7043\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 0.3666 Acc: 0.8426\n",
      "val Loss: 0.6952 Acc: 0.6996\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 0.3588 Acc: 0.8451\n",
      "val Loss: 1.9051 Acc: 0.5059\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 0.3589 Acc: 0.8449\n",
      "val Loss: 0.3506 Acc: 0.8559\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 0.3475 Acc: 0.8528\n",
      "val Loss: 0.4607 Acc: 0.7916\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 0.3449 Acc: 0.8513\n",
      "val Loss: 0.3464 Acc: 0.8484\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 0.3428 Acc: 0.8545\n",
      "val Loss: 0.3982 Acc: 0.8339\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 0.3404 Acc: 0.8564\n",
      "val Loss: 0.3519 Acc: 0.8518\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 0.3378 Acc: 0.8571\n",
      "val Loss: 0.4476 Acc: 0.8104\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 0.3354 Acc: 0.8568\n",
      "val Loss: 0.3380 Acc: 0.8545\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 0.3359 Acc: 0.8594\n",
      "val Loss: 0.4294 Acc: 0.8142\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 0.3235 Acc: 0.8636\n",
      "val Loss: 0.3474 Acc: 0.8526\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 0.3199 Acc: 0.8670\n",
      "val Loss: 0.6554 Acc: 0.7176\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 0.3191 Acc: 0.8669\n",
      "val Loss: 0.3337 Acc: 0.8594\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 0.3181 Acc: 0.8660\n",
      "val Loss: 0.2984 Acc: 0.8767\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 0.3164 Acc: 0.8670\n",
      "val Loss: 0.3624 Acc: 0.8451\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 0.3141 Acc: 0.8688\n",
      "val Loss: 0.6431 Acc: 0.7004\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 0.3114 Acc: 0.8703\n",
      "val Loss: 0.3041 Acc: 0.8730\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 0.3001 Acc: 0.8735\n",
      "val Loss: 0.8053 Acc: 0.6435\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 0.2976 Acc: 0.8762\n",
      "val Loss: 0.2941 Acc: 0.8819\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 0.2972 Acc: 0.8759\n",
      "val Loss: 0.3489 Acc: 0.8514\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 0.2929 Acc: 0.8798\n",
      "val Loss: 0.3395 Acc: 0.8548\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 0.2914 Acc: 0.8787\n",
      "val Loss: 0.2846 Acc: 0.8840\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 0.2902 Acc: 0.8810\n",
      "val Loss: 0.3475 Acc: 0.8522\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 0.2883 Acc: 0.8794\n",
      "val Loss: 0.2729 Acc: 0.8919\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 0.2771 Acc: 0.8869\n",
      "val Loss: 0.4621 Acc: 0.7916\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 0.2788 Acc: 0.8860\n",
      "val Loss: 0.2690 Acc: 0.8895\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 0.2709 Acc: 0.8895\n",
      "val Loss: 0.2594 Acc: 0.8962\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 0.2692 Acc: 0.8908\n",
      "val Loss: 0.2867 Acc: 0.8830\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 0.2659 Acc: 0.8926\n",
      "val Loss: 0.2580 Acc: 0.8993\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 0.2649 Acc: 0.8913\n",
      "val Loss: 0.2605 Acc: 0.8972\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 0.2606 Acc: 0.8956\n",
      "val Loss: 0.2669 Acc: 0.8845\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.2515 Acc: 0.8988\n",
      "val Loss: 0.2364 Acc: 0.9057\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 0.2476 Acc: 0.9027\n",
      "val Loss: 0.2475 Acc: 0.9026\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.2447 Acc: 0.9031\n",
      "val Loss: 0.2266 Acc: 0.9140\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.2473 Acc: 0.9010\n",
      "val Loss: 0.4130 Acc: 0.8158\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.2364 Acc: 0.9069\n",
      "val Loss: 0.2240 Acc: 0.9131\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.2371 Acc: 0.9060\n",
      "val Loss: 0.2299 Acc: 0.9108\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.2366 Acc: 0.9085\n",
      "val Loss: 0.2263 Acc: 0.9169\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.2272 Acc: 0.9133\n",
      "val Loss: 0.3346 Acc: 0.8613\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.2243 Acc: 0.9136\n",
      "val Loss: 0.2186 Acc: 0.9171\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.2194 Acc: 0.9166\n",
      "val Loss: 1.2427 Acc: 0.5352\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.2164 Acc: 0.9161\n",
      "val Loss: 0.2148 Acc: 0.9176\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.2166 Acc: 0.9162\n",
      "val Loss: 0.2365 Acc: 0.9080\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.2124 Acc: 0.9206\n",
      "val Loss: 0.2021 Acc: 0.9265\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.2140 Acc: 0.9182\n",
      "val Loss: 0.2323 Acc: 0.9123\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.2067 Acc: 0.9219\n",
      "val Loss: 0.2204 Acc: 0.9158\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.2022 Acc: 0.9253\n",
      "val Loss: 0.2108 Acc: 0.9214\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.2045 Acc: 0.9230\n",
      "val Loss: 0.2030 Acc: 0.9248\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.2007 Acc: 0.9252\n",
      "val Loss: 0.1886 Acc: 0.9347\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.1998 Acc: 0.9266\n",
      "val Loss: 0.1857 Acc: 0.9325\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.1968 Acc: 0.9265\n",
      "val Loss: 0.1889 Acc: 0.9283\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.1980 Acc: 0.9276\n",
      "val Loss: 0.1887 Acc: 0.9325\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.1917 Acc: 0.9309\n",
      "val Loss: 0.2478 Acc: 0.9018\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.1921 Acc: 0.9302\n",
      "val Loss: 0.2639 Acc: 0.8951\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.1883 Acc: 0.9312\n",
      "val Loss: 0.2637 Acc: 0.8974\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.1888 Acc: 0.9303\n",
      "val Loss: 0.1773 Acc: 0.9378\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.1853 Acc: 0.9330\n",
      "val Loss: 0.2281 Acc: 0.9120\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.1872 Acc: 0.9312\n",
      "val Loss: 1.0363 Acc: 0.6166\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.1859 Acc: 0.9339\n",
      "val Loss: 0.2079 Acc: 0.9225\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.1840 Acc: 0.9347\n",
      "val Loss: 0.1802 Acc: 0.9369\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.1834 Acc: 0.9347\n",
      "val Loss: 0.2047 Acc: 0.9241\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.1809 Acc: 0.9357\n",
      "val Loss: 0.2402 Acc: 0.9084\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.1812 Acc: 0.9358\n",
      "val Loss: 0.1785 Acc: 0.9360\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.1819 Acc: 0.9354\n",
      "val Loss: 0.1932 Acc: 0.9234\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.1815 Acc: 0.9357\n",
      "val Loss: 0.2017 Acc: 0.9252\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.1790 Acc: 0.9353\n",
      "val Loss: 0.1885 Acc: 0.9322\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.1794 Acc: 0.9362\n",
      "val Loss: 0.1862 Acc: 0.9341\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.1776 Acc: 0.9368\n",
      "val Loss: 0.1658 Acc: 0.9440\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.1781 Acc: 0.9369\n",
      "val Loss: 0.1638 Acc: 0.9450\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.1778 Acc: 0.9361\n",
      "val Loss: 0.1707 Acc: 0.9413\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.1756 Acc: 0.9377\n",
      "val Loss: 0.1708 Acc: 0.9420\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.1784 Acc: 0.9359\n",
      "val Loss: 0.1735 Acc: 0.9372\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.1773 Acc: 0.9369\n",
      "val Loss: 0.1927 Acc: 0.9288\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.1775 Acc: 0.9371\n",
      "val Loss: 0.4454 Acc: 0.8241\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.1774 Acc: 0.9366\n",
      "val Loss: 0.1642 Acc: 0.9434\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.1774 Acc: 0.9359\n",
      "val Loss: 0.1651 Acc: 0.9440\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.1767 Acc: 0.9366\n",
      "val Loss: 0.2148 Acc: 0.9208\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.1761 Acc: 0.9380\n",
      "val Loss: 0.1643 Acc: 0.9446\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.1749 Acc: 0.9372\n",
      "val Loss: 0.1809 Acc: 0.9341\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.1764 Acc: 0.9373\n",
      "val Loss: 0.9326 Acc: 0.6196\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.1749 Acc: 0.9389\n",
      "val Loss: 0.2868 Acc: 0.8873\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.1765 Acc: 0.9382\n",
      "val Loss: 0.7593 Acc: 0.7125\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.1751 Acc: 0.9371\n",
      "val Loss: 0.1634 Acc: 0.9435\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.1753 Acc: 0.9379\n",
      "val Loss: 0.7519 Acc: 0.7052\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.1766 Acc: 0.9369\n",
      "val Loss: 0.2596 Acc: 0.8981\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.1754 Acc: 0.9380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2071 Acc: 0.9233\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.1733 Acc: 0.9392\n",
      "val Loss: 0.1723 Acc: 0.9379\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.1758 Acc: 0.9373\n",
      "val Loss: 0.1827 Acc: 0.9332\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.1753 Acc: 0.9376\n",
      "val Loss: 0.1925 Acc: 0.9290\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.1776 Acc: 0.9365\n",
      "val Loss: 0.1633 Acc: 0.9452\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.1735 Acc: 0.9377\n",
      "val Loss: 0.2077 Acc: 0.9223\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.1763 Acc: 0.9364\n",
      "val Loss: 0.7127 Acc: 0.7327\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.1749 Acc: 0.9381\n",
      "val Loss: 0.1750 Acc: 0.9366\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.1745 Acc: 0.9383\n",
      "val Loss: 0.1631 Acc: 0.9436\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.1771 Acc: 0.9370\n",
      "val Loss: 0.3749 Acc: 0.8524\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.1756 Acc: 0.9370\n",
      "val Loss: 0.3030 Acc: 0.8809\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.1741 Acc: 0.9394\n",
      "val Loss: 0.1735 Acc: 0.9377\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.1749 Acc: 0.9385\n",
      "val Loss: 0.2574 Acc: 0.8978\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.1757 Acc: 0.9389\n",
      "val Loss: 0.2813 Acc: 0.8885\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.1756 Acc: 0.9379\n",
      "val Loss: 0.2152 Acc: 0.9198\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.1770 Acc: 0.9374\n",
      "val Loss: 0.1666 Acc: 0.9431\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.1755 Acc: 0.9377\n",
      "val Loss: 0.1924 Acc: 0.9306\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.1734 Acc: 0.9390\n",
      "val Loss: 0.1639 Acc: 0.9431\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.1753 Acc: 0.9377\n",
      "val Loss: 0.1702 Acc: 0.9401\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.1753 Acc: 0.9380\n",
      "val Loss: 0.2098 Acc: 0.9216\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.1761 Acc: 0.9365\n",
      "val Loss: 0.1989 Acc: 0.9171\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.1757 Acc: 0.9371\n",
      "val Loss: 0.2496 Acc: 0.9048\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.1771 Acc: 0.9357\n",
      "val Loss: 0.1736 Acc: 0.9386\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.1743 Acc: 0.9389\n",
      "val Loss: 0.3054 Acc: 0.8791\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.1774 Acc: 0.9365\n",
      "val Loss: 0.6831 Acc: 0.7308\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.1741 Acc: 0.9397\n",
      "val Loss: 0.1691 Acc: 0.9422\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.1745 Acc: 0.9401\n",
      "val Loss: 0.2814 Acc: 0.8898\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.1743 Acc: 0.9387\n",
      "val Loss: 0.1884 Acc: 0.9309\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.1729 Acc: 0.9379\n",
      "val Loss: 0.3156 Acc: 0.8745\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.1754 Acc: 0.9385\n",
      "val Loss: 0.2432 Acc: 0.9084\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.1744 Acc: 0.9387\n",
      "val Loss: 0.1685 Acc: 0.9423\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.1745 Acc: 0.9382\n",
      "val Loss: 0.2058 Acc: 0.9212\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.1735 Acc: 0.9392\n",
      "val Loss: 0.1695 Acc: 0.9409\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.1751 Acc: 0.9369\n",
      "val Loss: 0.5699 Acc: 0.7765\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.1753 Acc: 0.9384\n",
      "val Loss: 0.1611 Acc: 0.9477\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.1746 Acc: 0.9384\n",
      "val Loss: 0.1672 Acc: 0.9441\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.1748 Acc: 0.9385\n",
      "val Loss: 0.1812 Acc: 0.9352\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.1754 Acc: 0.9389\n",
      "val Loss: 0.1853 Acc: 0.9322\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.1749 Acc: 0.9386\n",
      "val Loss: 0.1699 Acc: 0.9400\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.1736 Acc: 0.9382\n",
      "val Loss: 0.2267 Acc: 0.9143\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.1755 Acc: 0.9379\n",
      "val Loss: 0.2379 Acc: 0.9095\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.1739 Acc: 0.9397\n",
      "val Loss: 0.1716 Acc: 0.9385\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.1733 Acc: 0.9401\n",
      "val Loss: 0.2666 Acc: 0.8948\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.1752 Acc: 0.9393\n",
      "val Loss: 0.1660 Acc: 0.9426\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.1746 Acc: 0.9382\n",
      "val Loss: 1.0456 Acc: 0.5852\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.1730 Acc: 0.9391\n",
      "val Loss: 0.3369 Acc: 0.8674\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.1741 Acc: 0.9380\n",
      "val Loss: 0.1731 Acc: 0.9400\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.1736 Acc: 0.9392\n",
      "val Loss: 0.1628 Acc: 0.9452\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.1744 Acc: 0.9386\n",
      "val Loss: 0.1712 Acc: 0.9392\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.1757 Acc: 0.9387\n",
      "val Loss: 0.1678 Acc: 0.9433\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.1745 Acc: 0.9378\n",
      "val Loss: 0.1775 Acc: 0.9368\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.1753 Acc: 0.9381\n",
      "val Loss: 0.2622 Acc: 0.8989\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.1744 Acc: 0.9393\n",
      "val Loss: 0.1890 Acc: 0.9306\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.1737 Acc: 0.9388\n",
      "val Loss: 0.1835 Acc: 0.9334\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.1758 Acc: 0.9371\n",
      "val Loss: 0.1778 Acc: 0.9387\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.1736 Acc: 0.9389\n",
      "val Loss: 1.0022 Acc: 0.6556\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.1735 Acc: 0.9396\n",
      "val Loss: 0.2457 Acc: 0.8910\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.1743 Acc: 0.9382\n",
      "val Loss: 0.1753 Acc: 0.9377\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.1728 Acc: 0.9398\n",
      "val Loss: 0.1877 Acc: 0.9321\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.1761 Acc: 0.9370\n",
      "val Loss: 0.1726 Acc: 0.9398\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.1743 Acc: 0.9398\n",
      "val Loss: 0.1733 Acc: 0.9410\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.1755 Acc: 0.9372\n",
      "val Loss: 0.1724 Acc: 0.9399\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.1750 Acc: 0.9383\n",
      "val Loss: 0.1730 Acc: 0.9397\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.1729 Acc: 0.9386\n",
      "val Loss: 0.1886 Acc: 0.9308\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.1755 Acc: 0.9375\n",
      "val Loss: 0.2236 Acc: 0.9149\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.1741 Acc: 0.9390\n",
      "val Loss: 0.1846 Acc: 0.9340\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.1746 Acc: 0.9383\n",
      "val Loss: 0.1702 Acc: 0.9413\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.1739 Acc: 0.9396\n",
      "val Loss: 0.1757 Acc: 0.9374\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.1738 Acc: 0.9391\n",
      "val Loss: 0.1748 Acc: 0.9376\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.1745 Acc: 0.9399\n",
      "val Loss: 0.2346 Acc: 0.9113\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.1746 Acc: 0.9389\n",
      "val Loss: 0.2106 Acc: 0.9175\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.1753 Acc: 0.9381\n",
      "val Loss: 0.2205 Acc: 0.9170\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.1747 Acc: 0.9382\n",
      "val Loss: 0.2395 Acc: 0.9075\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.1734 Acc: 0.9387\n",
      "val Loss: 0.5434 Acc: 0.7797\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.1745 Acc: 0.9390\n",
      "val Loss: 0.2343 Acc: 0.9125\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.1728 Acc: 0.9390\n",
      "val Loss: 0.2087 Acc: 0.9221\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.1757 Acc: 0.9369\n",
      "val Loss: 0.2776 Acc: 0.8919\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.1746 Acc: 0.9387\n",
      "val Loss: 0.2387 Acc: 0.9065\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.1723 Acc: 0.9387\n",
      "val Loss: 0.5687 Acc: 0.7744\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.1755 Acc: 0.9372\n",
      "val Loss: 0.1710 Acc: 0.9414\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.1725 Acc: 0.9398\n",
      "val Loss: 1.1809 Acc: 0.5725\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.1745 Acc: 0.9385\n",
      "val Loss: 1.1994 Acc: 0.5642\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.1763 Acc: 0.9375\n",
      "val Loss: 0.1681 Acc: 0.9414\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.1744 Acc: 0.9376\n",
      "val Loss: 0.2084 Acc: 0.9226\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.1744 Acc: 0.9387\n",
      "val Loss: 0.1905 Acc: 0.9318\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.1743 Acc: 0.9377\n",
      "val Loss: 0.3036 Acc: 0.8803\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.1737 Acc: 0.9382\n",
      "val Loss: 0.2634 Acc: 0.8954\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.1759 Acc: 0.9380\n",
      "val Loss: 0.1804 Acc: 0.9353\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.1727 Acc: 0.9380\n",
      "val Loss: 0.1597 Acc: 0.9493\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.1760 Acc: 0.9380\n",
      "val Loss: 0.1932 Acc: 0.9299\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.1744 Acc: 0.9382\n",
      "val Loss: 0.1837 Acc: 0.9342\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.1742 Acc: 0.9391\n",
      "val Loss: 0.1869 Acc: 0.9338\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.1741 Acc: 0.9386\n",
      "val Loss: 0.1750 Acc: 0.9377\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.1762 Acc: 0.9387\n",
      "val Loss: 0.1767 Acc: 0.9363\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.1758 Acc: 0.9377\n",
      "val Loss: 0.1725 Acc: 0.9408\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.1734 Acc: 0.9392\n",
      "val Loss: 0.2677 Acc: 0.8973\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1747 Acc: 0.9381\n",
      "val Loss: 4.0930 Acc: 0.5050\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.1759 Acc: 0.9383\n",
      "val Loss: 0.1816 Acc: 0.9341\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.1730 Acc: 0.9391\n",
      "val Loss: 0.1656 Acc: 0.9419\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.1763 Acc: 0.9375\n",
      "val Loss: 0.1810 Acc: 0.9368\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.1746 Acc: 0.9380\n",
      "val Loss: 0.4836 Acc: 0.8068\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.1743 Acc: 0.9388\n",
      "val Loss: 0.1795 Acc: 0.9374\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.1744 Acc: 0.9384\n",
      "val Loss: 0.2248 Acc: 0.9116\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.1746 Acc: 0.9388\n",
      "val Loss: 0.1615 Acc: 0.9483\n",
      "\n",
      "Training complete in 560m 14s\n",
      "Best val Acc: 0.949320\n",
      "[[22219  2103]\n",
      " [   30  1984]]\n",
      "Current hyperparameters: {'learning rate': 0.1, 'gamma': 0.5, 'Momentum': 0.9, 'Model': 'model18'}\n",
      "Best hyperparameters: {'learning rate': 0.1, 'gamma': 0.5, 'Momentum': 0.9, 'Model': 'model18'}\n",
      "Best score: 0.94931966\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_weights= [0.924865,0.075135] #1-(#inclass/ #intotal )\n",
    "class_weights = torch.Tensor(class_weights)\n",
    "class_weights=class_weights.to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight = class_weights) \n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "learning_rate=[0.05, 0.1,0.2]\n",
    "Momentum=[0.5,0.9,0.99]\n",
    "GAMMA=[0.05, 0.1, 0.5]\n",
    "#Batch_size= [16,32]#,64,128]\n",
    "models=[model18, model34, model50]\n",
    "modelnames=[\"model18\",\"model34\",\"model50\"]\n",
    "best_params = None\n",
    "best_score=0.00\n",
    "Current_params=None\n",
    "lrnum=1\n",
    "Momnum=1\n",
    "gamnum=2\n",
    "modnum=0\n",
    "\n",
    "model=models[modnum]\n",
    "model = model.to(device)\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=learning_rate[lrnum], momentum=Momentum[Momnum])\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=GAMMA[gamnum])\n",
    "writer = SummaryWriter(\"Experiments/200againbest_model_params/lr\"+str(learning_rate[lrnum])+\"momentum\"+\n",
    "        str(Momentum[Momnum])+\"gamma\"+str(GAMMA[gamnum])+\"model\"+modelnames[modnum])  \n",
    "model_ft, con_mat, current_score = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "   train,test,num_epochs=200)\n",
    "con_mat=con_mat.cpu().numpy()\n",
    "current_score=current_score.cpu().numpy()\n",
    "Current_params= {'learning rate': learning_rate[lrnum], 'gamma': GAMMA[gamnum], \n",
    "                   'Momentum': Momentum[Momnum], 'Model': modelnames[modnum]}\n",
    "print(con_mat)\n",
    "print(\"Current hyperparameters:\", Current_params)\n",
    "df_cm_ratio = pd.DataFrame(con_mat / np.sum(con_mat, axis=1)[:, None], index=[i for i in labels],\n",
    "                     columns=[i for i in labels])\n",
    "plt.figure(figsize=(12, 7))\n",
    "sn.heatmap(df_cm_ratio, annot=True)\n",
    "if current_score > best_score:\n",
    "    best_score = current_score\n",
    "    best_params = {'learning rate': learning_rate[lrnum], 'gamma': GAMMA[gamnum], \n",
    "                   'Momentum': Momentum[Momnum], 'Model': modelnames[modnum]}\n",
    "                            \n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best score:\", best_score)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cf_matrix = confusion_matrix_calc(data_loader_test, labels, model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-424431bc048bf7b1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-424431bc048bf7b1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir Experiments/200againbest_model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning rate': 0.1, 'gamma': 0.5, 'Momentum': 0.9, 'Model': 'model18'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(0.94931966, dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_params)\n",
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate=[0.05, 0.1,0.2]\n",
    "learning_rate[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), \"/workspace/myFile/Output/17052023/200againbest_model_params.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2203/4163751851.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
