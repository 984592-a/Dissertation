{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To note: Running this on different systems (i.e. local, SCW, server) will result in slight changes needing to the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x7fe0317c8820>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from PIL import ImageEnhance\n",
    "import matplotlib.pyplot as plt\n",
    "import imagesize\n",
    "import subprocess\n",
    "sys.path\n",
    "from IPython.core.debugger import set_trace\n",
    "import scipy.ndimage\n",
    "import matplotlib.patches as patches\n",
    "# plt.rcParams['figure.figsize'] = [12,12]\n",
    "# sys.path.append('/workspace/myFile/Mask_RCNN_Tutorial/')\n",
    "from tqdm import tqdm\n",
    "from torch import nn as nn\n",
    "from torch import optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import re\n",
    "import time\n",
    "import copy\n",
    "import pylab\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tempfile import TemporaryDirectory\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import glob\n",
    "# Rather than have a messy notebook with a load of functions we can store them in separate .py files and import them\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "writer = SummaryWriter(\"Experiments/TENSORBOARD\")    # This determines the tensorboard file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNADataset(object):\n",
    "    def __init__(self, root, transforms, labels, imDx = False):\n",
    "        self.root, self.transforms, self.labels = root, transforms, labels\n",
    "    \n",
    "        # load all image files, sorting them to ensure they are aligned\n",
    "        self.imgDir = glob.glob(root+\"*.tiff\")\n",
    "        Damagednuclei= [x for x in self.imgDir if 'Damaged_nuclei_' in x]\n",
    "        Undamagednuclei= [x for x in self.imgDir if \"No_damage_nuclei\" in x]\n",
    "        #np.random.shuffle(Undamagednuclei)\n",
    "        #Undamagednuclei=Undamagednuclei[:10000]\n",
    "        self.imgDir= Damagednuclei+Undamagednuclei\n",
    "        size80=[]\n",
    "        for x in self.imgDir:\n",
    "            img = Image.open(x) # Open image\n",
    "            w,h=img.size\n",
    "            if w<=80 and h<=80:\n",
    "                size80.append(x)\n",
    "        self.imgDir=size80              \n",
    "        \n",
    "        self.imgs = sorted(self.imgDir) # list of images\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.imgs[idx]\n",
    "       \n",
    "        # Transform images into tensors\n",
    "        img = Image.open(img_path) # Open image\n",
    "        w,h=img.size\n",
    "        img = np.array(img) # Convert image into an array\n",
    "        img = np.float32(np.divide(img, 2**16)) # Ensure all values are floats\n",
    "        \n",
    "        result=np.zeros((80,80), dtype=np.float32)\n",
    "        x_center = (80 - w) // 2\n",
    "        y_center = (80 - h) // 2 # copy img image into center of result image\n",
    "        result[y_center:y_center+h, x_center:x_center+w] = img\n",
    "        img = result\n",
    "        \n",
    "        targetlab=\"\"\n",
    "        if img_path.find('No_damage_nuclei') != -1:\n",
    "            targetlab= 'Undamaged'\n",
    "        if img_path.find('Damaged_nuclei_') != -1:\n",
    "            targetlab= 'Damaged'  # Find labels corresponding to image\n",
    "        target = self.labels.index(targetlab) # Get the label and assign to a value\n",
    "        \n",
    "        # Convert label to tensor\n",
    "        #torch.to\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "#             #print('In the transforms')\n",
    "        imNo = idx\n",
    "        return img, target, imNo\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imDr = \"/workspace/myFile/Output/11082023/A0101/\"  # Image patches directory\n",
    "\n",
    "labels = ['Undamaged','Damaged']  # Your labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data augmentation\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "\n",
    "    transforms.append(T.ToTensor())\n",
    "    #transforms.append(T.Normalize([0.0019368887995516483], [0.00672996630111016]))\n",
    "    #transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    \n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(p=1))\n",
    "        transforms.append(T.RandomVerticalFlip(p=1))\n",
    "    \n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, train, test, num_epochs=25):\n",
    "    since = time.time()\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "            dataset_train = torch.utils.data.Subset(dataSetTrain, train[epoch])\n",
    "            dataset_test = torch.utils.data.Subset(dataSetTest, test[epoch])\n",
    "            data_loader_train = torch.utils.data.DataLoader(\n",
    "                dataset_train, batch_size=128, shuffle=True, num_workers=0)\n",
    "\n",
    "            data_loader_test = torch.utils.data.DataLoader(\n",
    "                dataset_test, batch_size=128, shuffle=False, num_workers=0)\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                    dataloaders = data_loader_train\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "                    dataloaders = data_loader_test\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "                damaged_len=0\n",
    "                damaged_corrects = 0\n",
    "                undamaged_len=0\n",
    "                undamaged_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels, imNo in dataloaders:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)    # Loss\n",
    "                    nada=torch.tensor(np.zeros(len(labels))).to(device)\n",
    "                    uno=nada+1\n",
    "                    falseneg=preds+1\n",
    "                    falsepos=preds-1\n",
    "                    FPplusTN=torch.sum(labels==nada)\n",
    "                    TPplusFN=torch.sum(labels==uno)\n",
    "                    FN=torch.sum(labels==falseneg)\n",
    "                    FP=torch.sum(labels==falsepos)\n",
    "                    TN=FPplusTN-FP\n",
    "                    TP=TPplusFN-FN\n",
    "                    running_corrects += torch.sum(preds == labels.data) # Accuracy\n",
    "                    damaged_len+=TPplusFN\n",
    "                    damaged_corrects += TP\n",
    "                    undamaged_len+=FPplusTN\n",
    "                    undamaged_corrects += TN\n",
    "                damaged_acc = damaged_corrects / damaged_len\n",
    "                undamaged_acc = undamaged_corrects/undamaged_len\n",
    "                epoch_acc= (damaged_acc+undamaged_acc)/2\n",
    "                proper_acc=(damaged_corrects+undamaged_corrects)/(damaged_len+undamaged_len)\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]    # Loss metric per epoch\n",
    "                #epoch_acc = running_corrects.double() / dataset_sizes[phase]    # Accuracy metric per epoch\n",
    "\n",
    "                if phase == \"train\":    # This is the tensorboard code that writes accuracy and loss metrics\n",
    "                    writer.add_scalar(\"Train/Accuracy\", epoch_acc, epoch)\n",
    "                    writer.add_scalar(\"Train/Loss\", epoch_loss, epoch)\n",
    "                else:\n",
    "                    writer.add_scalar(\"Validation/Accuracy\", epoch_acc, epoch)\n",
    "                    writer.add_scalar(\"Validation/Loss\", epoch_loss, epoch)\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "                print(\"proper accuracy=\")\n",
    "                print(proper_acc)\n",
    "                print(\"damaged accuracy=\")\n",
    "                print(damaged_acc)\n",
    "                print(\"undamaged accuracy=\")\n",
    "                print(undamaged_acc)\n",
    "                con_matter= torch.tensor([[damaged_corrects, damaged_len-damaged_corrects],[undamaged_len-undamaged_corrects,undamaged_corrects]])\n",
    "                con_matter=con_matter.cpu().numpy()\n",
    "                print(con_matter)\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and proper_acc >= best_acc: \n",
    "                    # This compares validation accuracy to previous bests and adjusts model weights accordingly\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "                    con_mat= torch.tensor([[undamaged_len-undamaged_corrects,undamaged_corrects],[damaged_corrects, damaged_len-damaged_corrects]])\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since  # Nice way to measure training time but info also stored (indirectly) by tensorboard\n",
    "        print(\n",
    "            f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "        #labels= labels.cpu().numpy()\n",
    "        #preds=preds.cpu().numpy()\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    writer.close()\n",
    "    return model, con_mat   # We want to return the model because its the model, also confusion matrix for later analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is your RESNET\n",
    "# Initialize CNN with kaiming\n",
    "def init_cnn(m):\n",
    "    # Set the weights of the RESNET\n",
    "    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)\n",
    "    if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight)\n",
    "    for l in m.children(): init_cnn(l)\n",
    "\n",
    "\n",
    "# noop function for returning nothing\n",
    "def noop(x): return x\n",
    "# activation function(RELU)\n",
    "act_fn = nn.ReLU(inplace=True)\n",
    "\n",
    "# Flatten\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): return x.view(x.size(0), -1)\n",
    "\n",
    "# Make a convolution\n",
    "def conv(ni, nf, ks=3, stride=1, bias=False):\n",
    "    return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)\n",
    "\n",
    "# Create a convuolutional layer with convolution and batch norm\n",
    "def conv_layer(ni, nf, ks=3, stride=1, zero_bn=False, act=True):\n",
    "    bn = nn.BatchNorm2d(nf) # get a 2d batch norm from Pytorhc\n",
    "    nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n",
    "    layers = [conv(ni, nf, ks, stride=stride), bn]\n",
    "    if act: layers.append(act_fn) # add in the activation function if act is true\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "# Resblock\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, expansion, ni, nh, stride = 1):\n",
    "        super().__init__()\n",
    "        # ni - number of inputs channels, nf - number of filters\n",
    "        # nh - number of filters in first conv\n",
    "        # expansion is 1 for resnet 18, 34 and 4 for larger networks\n",
    "        nf, ni = nh*expansion, ni*expansion\n",
    "        layers = [conv_layer(ni, nh, 3, stride = stride), # for resnet < 34 2 convs per resblock\n",
    "                 conv_layer(nh, nf, 3, zero_bn = True, act = False)\n",
    "                 ] if expansion == 1 else [ # for RESNET > 34 then 3 convs per block with bottleneck\n",
    "                            conv_layer(ni, nh, 1),\n",
    "                            conv_layer(nh, nh, 3, stride = stride),\n",
    "                            conv_layer(nh, nf, 1, zero_bn = True, act = False)\n",
    "        ]\n",
    "        self.convs = nn.Sequential(*layers) # Creates the conv layers\n",
    "        self.idconv = noop if ni==nf else conv_layer(ni, nf, 1, act = False) # id convolution ()\n",
    "        self.pool = noop if stride== 1 else nn.AvgPool2d(2, ceil_mode = True) # average pool on \n",
    "        \n",
    "    def forward(self, x): \n",
    "        # Forward function adds the convolution part to the id part \n",
    "        #return act_fn(self.convs(x)) + self.idconv(self.pool(x))\n",
    "        return act_fn(self.convs(x) + self.idconv(self.pool(x)))\n",
    "\n",
    "# XResnet\n",
    "class XResNet(nn.Sequential):\n",
    "    @classmethod\n",
    "    def create(cls, expansion, layers, c_in=3, c_out=1000):\n",
    "        nfs = [c_in, (c_in + 1)*8, 64, 64] # number of filters in stem layer (c_in is number of image channels)\n",
    "        stem = [conv_layer(nfs[i], nfs[i+1], stride=2 if i==0 else 1)\n",
    "            for i in range(3)]\n",
    "\n",
    "        nfs = [64//expansion,64,128,256,512]\n",
    "        res_layers = [cls._make_layer(expansion, nfs[i], nfs[i+1],\n",
    "                                      n_blocks=l, stride=1 if i==0 else 2)\n",
    "                  for i,l in enumerate(layers)]\n",
    "        res = cls(\n",
    "        *stem,\n",
    "        nn.MaxPool2d(kernel_size=3, stride = 2, padding = 1), # then a max pooling layer\n",
    "        *res_layers,\n",
    "        nn.AdaptiveAvgPool2d(1), Flatten(), \n",
    "        nn.Linear(nfs[-1]*expansion, c_out)\n",
    "        )\n",
    "        init_cnn(res)\n",
    "        return res\n",
    "        \n",
    "    @staticmethod\n",
    "    def _make_layer(expansion, ni, nf, n_blocks, stride): # returns a resblock\n",
    "        return nn.Sequential(\n",
    "        *[ResBlock(expansion, ni if i==0 else nf, nf, stride if i==0 else 1)\n",
    "         for i in range(n_blocks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xresnet18 (**kwargs): return XResNet.create(1, [2, 2,  2, 2], **kwargs)\n",
    "def xresnet34 (**kwargs): return XResNet.create(1, [3, 4,  6, 3], **kwargs)\n",
    "def xresnet50 (**kwargs): return XResNet.create(4, [3, 4,  6, 3], **kwargs)\n",
    "model = xresnet18(c_in = 1, c_out = 2)\n",
    "#model34 = xresnet34(c_in = 1, c_out = 2)\n",
    "#model50 = xresnet50(c_in = 1, c_out = 2)\n",
    "model.load_state_dict(torch.load(\"/workspace/myFile/Output/17052023/best_model_params.pt\"))\n",
    "model = model.to(device)\n",
    "\n",
    "# Label smoothing cross entropy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def reduce_loss(loss, reduction='mean'):\n",
    "    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, ε:float=0.1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.ε,self.reduction = ε,reduction\n",
    "    \n",
    "    def forward(self, output, target):\n",
    "        c = output.size()[-1]\n",
    "        log_preds = F.log_softmax(output, dim=-1)\n",
    "        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n",
    "        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n",
    "        return lin_comb(loss/c, nll, self.ε)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions that shows the image, true class, predicted class and degree of prediction\n",
    "\n",
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    '''\n",
    "    output = net(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            labels[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            labels[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig\n",
    "\n",
    "def matplotlib_imshow(img, one_channel=True):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_calc(data_loader_test, classes, model_ft):       \n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for inputs, labels, imNo in data_loader_test:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                output = model_ft(inputs) # Feed Network\n",
    "\n",
    "                output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "                y_pred.extend(output) # Save Prediction\n",
    "                \n",
    "                labels = labels.data.cpu().numpy()\n",
    "                y_true.extend(labels) # Save Truth\n",
    "\n",
    "        # constant for classes\n",
    "        # classes = ('Alive', 'Dead')\n",
    "\n",
    "        # Build confusion matrix\n",
    "        cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "        return cf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5 #NUMBER OF K FOLDS\n",
    "epoch=range(5) #NUMBER OF EPOCHS\n",
    "\n",
    "torch.manual_seed(10)\n",
    "imIdx = torch.randperm(1).tolist()\n",
    "\n",
    "## Create dataset\n",
    "dataSetTrain = DNADataset(imDr, get_transform(train = True), labels, imDx=imIdx)\n",
    "dataSetTest = DNADataset(imDr, get_transform(train = False), labels, imDx=imIdx)\n",
    "\n",
    "data_loader_train=[]\n",
    "data_loader_test=[]\n",
    "\n",
    "# ## Create dataloaders\n",
    "# Get subset\n",
    "torch.manual_seed(10)\n",
    "indices = torch.randperm(len(dataSetTrain)).tolist()\n",
    "\n",
    "noTrain = int(len(dataSetTrain)*0.7)\n",
    "train=[]\n",
    "test=[]\n",
    "for epoch in epoch:\n",
    "    indices = torch.randperm(len(dataSetTrain)).tolist()\n",
    "    TotalSet=list(range(len(dataSetTrain)))\n",
    "    Kfifth = len(dataSetTrain)//k\n",
    "    split= epoch%k\n",
    "    train.append(TotalSet[(split+1)*Kfifth:]+TotalSet[:Kfifth*split])\n",
    "\n",
    "    indices = torch.randperm(len(dataSetTest)).tolist()\n",
    "    TotalSet=list(range(len(dataSetTrain)))\n",
    "    test.append(TotalSet[split*Kfifth:(split+1)*Kfifth])\n",
    "\n",
    "#dataset_train = torch.utils.data.Subset(dataSetTrain, indices[-noTrain:])\n",
    "\n",
    "#dataset_test = torch.utils.data.Subset(dataSetTest, indices[:-noTrain])\n",
    "#len(indices), len(indices[:-50]), len(indices[-50:]), 50/191, type(dataset_test)\n",
    "\n",
    "dataset_sizes = {'train': len(train[0]), 'val': len(test[0])}\n",
    "\n",
    "#dataset_train = torch.utils.data.Subset(dataSetTrain, train)\n",
    "#data_loader_train.append(dataset_train)\n",
    "#dataset_testo = torch.utils.data.Subset(dataSetTest, test)\n",
    "#data_loader_test.append(dataset_testo)\n",
    "# define training and validation data loaders\n",
    "\n",
    "\n",
    "# Collate function (gathers together the outputs)\n",
    "# def collate_fn(batch):\n",
    "#     return tuple(zip(*batch))\n",
    "\n",
    "#len(indices[-noTrain:]), dataset_sizes\n",
    "#dataset_test[0][1], dataset_test[3][1], dataset_test[-1][1], dataSetTrain.imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights= [0.924865,0.075135] #1-(#inclass/ #intotal )\n",
    "class_weights = torch.Tensor(class_weights)\n",
    "class_weights=class_weights.to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight = class_weights) \n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model.parameters(), weight_decay=1e-2) # standard ADAM optimiser\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n",
      "val Loss: 3.7065 Acc: nan\n",
      "proper accuracy=\n",
      "tensor(0.1389, device='cuda:0')\n",
      "damaged accuracy=\n",
      "tensor(nan, device='cuda:0')\n",
      "undamaged accuracy=\n",
      "tensor(0.1389, device='cuda:0')\n",
      "[[ 0  0]\n",
      " [31  5]]\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "val Loss: 4.5145 Acc: nan\n",
      "proper accuracy=\n",
      "tensor(0., device='cuda:0')\n",
      "damaged accuracy=\n",
      "tensor(nan, device='cuda:0')\n",
      "undamaged accuracy=\n",
      "tensor(0., device='cuda:0')\n",
      "[[ 0  0]\n",
      " [36  0]]\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "val Loss: 3.9469 Acc: nan\n",
      "proper accuracy=\n",
      "tensor(0.1389, device='cuda:0')\n",
      "damaged accuracy=\n",
      "tensor(nan, device='cuda:0')\n",
      "undamaged accuracy=\n",
      "tensor(0.1389, device='cuda:0')\n",
      "[[ 0  0]\n",
      " [31  5]]\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "val Loss: 3.7620 Acc: nan\n",
      "proper accuracy=\n",
      "tensor(0.1667, device='cuda:0')\n",
      "damaged accuracy=\n",
      "tensor(nan, device='cuda:0')\n",
      "undamaged accuracy=\n",
      "tensor(0.1667, device='cuda:0')\n",
      "[[ 0  0]\n",
      " [30  6]]\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "val Loss: 3.4741 Acc: nan\n",
      "proper accuracy=\n",
      "tensor(0.1111, device='cuda:0')\n",
      "damaged accuracy=\n",
      "tensor(nan, device='cuda:0')\n",
      "undamaged accuracy=\n",
      "tensor(0.1111, device='cuda:0')\n",
      "[[ 0  0]\n",
      " [32  4]]\n",
      "\n",
      "Training complete in 0m 3s\n",
      "Best val Acc:  nan\n"
     ]
    }
   ],
   "source": [
    "model_ft, con_mat = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, \n",
    "                                    train,test,num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cf_matrix = confusion_matrix_calc(data_loader_test, labels, model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4710/4234404813.py:2: RuntimeWarning: invalid value encountered in divide\n",
      "  df_cm_ratio = pd.DataFrame(con_mat / np.sum(con_mat, axis=1)[:, None], index=[i for i in labels],\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "con_mat=con_mat.cpu().numpy()\n",
    "df_cm_ratio = pd.DataFrame(con_mat / np.sum(con_mat, axis=1)[:, None], index=[i for i in labels],\n",
    "                     columns=[i for i in labels])\n",
    "plt.figure(figsize=(12, 7))\n",
    "sn.heatmap(df_cm_ratio, annot=True)\n",
    "# plt.save(\"Confusion_matrix_ratio.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_cm_raw = pd.DataFrame(con_mat, index=[i for i in labels],\n",
    "                     columns=[i for i in labels])\n",
    "plt.figure(figsize=(12, 7))\n",
    "sn.heatmap(df_cm_raw, annot=True)\n",
    "# plt.save(\"Confusion_matrix_raw.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[31,  5],\n",
       "       [ 0,  0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
