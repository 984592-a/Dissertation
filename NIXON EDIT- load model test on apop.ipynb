{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To note: Running this on different systems (i.e. local, SCW, server) will result in slight changes needing to the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x7f46ce727f70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from PIL import ImageEnhance\n",
    "import matplotlib.pyplot as plt\n",
    "import imagesize\n",
    "import subprocess\n",
    "sys.path\n",
    "from IPython.core.debugger import set_trace\n",
    "import scipy.ndimage\n",
    "import matplotlib.patches as patches\n",
    "# plt.rcParams['figure.figsize'] = [12,12]\n",
    "# sys.path.append('/workspace/myFile/Mask_RCNN_Tutorial/')\n",
    "from tqdm import tqdm\n",
    "from torch import nn as nn\n",
    "from torch import optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import re\n",
    "import time\n",
    "import copy\n",
    "import pylab\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tempfile import TemporaryDirectory\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import glob\n",
    "# Rather than have a messy notebook with a load of functions we can store them in separate .py files and import them\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "writer = SummaryWriter(\"Experiments/TENSORBOARD\")    # This determines the tensorboard file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNADataset(object):\n",
    "    def __init__(self, root, transforms, labels, imDx = False):\n",
    "        self.root, self.transforms, self.labels = root, transforms, labels\n",
    "    \n",
    "        # load all image files, sorting them to ensure they are aligned\n",
    "        self.imgDir = glob.glob(root+\"*/*.tiff\")\n",
    "        Damagednuclei= [x for x in self.imgDir if 'Damaged_nuclei_' in x]\n",
    "        Undamagednuclei= [x for x in self.imgDir if \"No_damage_nuclei\" in x]\n",
    "        #np.random.shuffle(Undamagednuclei)\n",
    "        #Undamagednuclei=Undamagednuclei[:10000]\n",
    "        self.imgDir= Damagednuclei+Undamagednuclei\n",
    "        size80=[]\n",
    "        for x in self.imgDir:\n",
    "            img = Image.open(x) # Open image\n",
    "            w,h=img.size\n",
    "            if w<=80 and h<=80:\n",
    "                size80.append(x)\n",
    "        self.imgDir=size80              \n",
    "        \n",
    "        self.imgs = sorted(self.imgDir) # list of images\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.imgs[idx]\n",
    "       \n",
    "        # Transform images into tensors\n",
    "        img = Image.open(img_path) # Open image\n",
    "        w,h=img.size\n",
    "        img = np.array(img) # Convert image into an array\n",
    "        img = np.float32(np.divide(img, 2**16)) # Ensure all values are floats\n",
    "        \n",
    "        result=np.zeros((80,80), dtype=np.float32)\n",
    "        x_center = (80 - w) // 2\n",
    "        y_center = (80 - h) // 2 # copy img image into center of result image\n",
    "        result[y_center:y_center+h, x_center:x_center+w] = img\n",
    "        img = result\n",
    "        \n",
    "        targetlab=\"\"\n",
    "        if img_path.find('No_damage_nuclei') != -1:\n",
    "            targetlab= 'Undamaged'\n",
    "        if img_path.find('Damaged_nuclei_') != -1:\n",
    "            targetlab= 'Damaged'  # Find labels corresponding to image\n",
    "        target = self.labels.index(targetlab) # Get the label and assign to a value\n",
    "        \n",
    "        # Convert label to tensor\n",
    "        #torch.to\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "#             #print('In the transforms')\n",
    "        imNo = idx\n",
    "        return img, target, imNo\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imDr = \"/workspace/myFile/Output/11082023/\"  # Image patches directory\n",
    "\n",
    "labels = ['Undamaged','Damaged']  # Your labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data augmentation\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "\n",
    "    transforms.append(T.ToTensor())\n",
    "    #transforms.append(T.Normalize([0.0019368887995516483], [0.00672996630111016]))\n",
    "    #transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    \n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(p=1))\n",
    "        transforms.append(T.RandomVerticalFlip(p=1))\n",
    "    \n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, train, test, num_epochs=25):\n",
    "    since = time.time()\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "            dataset_train = torch.utils.data.Subset(dataSetTrain, train[epoch])\n",
    "            dataset_test = torch.utils.data.Subset(dataSetTest, test[epoch])\n",
    "            data_loader_train = torch.utils.data.DataLoader(\n",
    "                dataset_train, batch_size=128, shuffle=True, num_workers=0)\n",
    "\n",
    "            data_loader_test = torch.utils.data.DataLoader(\n",
    "                dataset_test, batch_size=128, shuffle=False, num_workers=0)\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                    dataloaders = data_loader_train\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "                    dataloaders = data_loader_test\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "                damaged_len=0\n",
    "                damaged_corrects = 0\n",
    "                undamaged_len=0\n",
    "                undamaged_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels, imNo in dataloaders:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)    # Loss\n",
    "                    nada=torch.tensor(np.zeros(len(labels))).to(device)\n",
    "                    uno=nada+1\n",
    "                    falseneg=preds+1\n",
    "                    falsepos=preds-1\n",
    "                    FPplusTN=torch.sum(labels==nada)\n",
    "                    TPplusFN=torch.sum(labels==uno)\n",
    "                    FN=torch.sum(labels==falseneg)\n",
    "                    FP=torch.sum(labels==falsepos)\n",
    "                    TN=FPplusTN-FP\n",
    "                    TP=TPplusFN-FN\n",
    "                    running_corrects += torch.sum(preds == labels.data) # Accuracy\n",
    "                    damaged_len+=TPplusFN\n",
    "                    damaged_corrects += TP\n",
    "                    undamaged_len+=FPplusTN\n",
    "                    undamaged_corrects += TN\n",
    "                damaged_acc = damaged_corrects / damaged_len\n",
    "                undamaged_acc = undamaged_corrects/undamaged_len\n",
    "                epoch_acc= (damaged_acc+undamaged_acc)/2\n",
    "                proper_acc=(damaged_corrects+undamaged_corrects)/(damaged_len+undamaged_len)\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]    # Loss metric per epoch\n",
    "                #epoch_acc = running_corrects.double() / dataset_sizes[phase]    # Accuracy metric per epoch\n",
    "\n",
    "                if phase == \"train\":    # This is the tensorboard code that writes accuracy and loss metrics\n",
    "                    writer.add_scalar(\"Train/Accuracy\", epoch_acc, epoch)\n",
    "                    writer.add_scalar(\"Train/Loss\", epoch_loss, epoch)\n",
    "                else:\n",
    "                    writer.add_scalar(\"Validation/Accuracy\", epoch_acc, epoch)\n",
    "                    writer.add_scalar(\"Validation/Loss\", epoch_loss, epoch)\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "                print(\"proper accuracy=\")\n",
    "                print(proper_acc)\n",
    "                print(\"damaged accuracy=\")\n",
    "                print(damaged_acc)\n",
    "                print(\"undamaged accuracy=\")\n",
    "                print(undamaged_acc)\n",
    "                con_matter= torch.tensor([[damaged_corrects, damaged_len-damaged_corrects],[undamaged_len-undamaged_corrects,undamaged_corrects]])\n",
    "                con_matter=con_matter.cpu().numpy()\n",
    "                print(con_matter)\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and proper_acc >= best_acc: \n",
    "                    # This compares validation accuracy to previous bests and adjusts model weights accordingly\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "                    con_mat= torch.tensor([[undamaged_len-undamaged_corrects,undamaged_corrects],[damaged_corrects, damaged_len-damaged_corrects]])\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since  # Nice way to measure training time but info also stored (indirectly) by tensorboard\n",
    "        print(\n",
    "            f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "        #labels= labels.cpu().numpy()\n",
    "        #preds=preds.cpu().numpy()\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    writer.close()\n",
    "    return model, con_mat   # We want to return the model because its the model, also confusion matrix for later analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is your RESNET\n",
    "# Initialize CNN with kaiming\n",
    "def init_cnn(m):\n",
    "    # Set the weights of the RESNET\n",
    "    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)\n",
    "    if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight)\n",
    "    for l in m.children(): init_cnn(l)\n",
    "\n",
    "\n",
    "# noop function for returning nothing\n",
    "def noop(x): return x\n",
    "# activation function(RELU)\n",
    "act_fn = nn.ReLU(inplace=True)\n",
    "\n",
    "# Flatten\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): return x.view(x.size(0), -1)\n",
    "\n",
    "# Make a convolution\n",
    "def conv(ni, nf, ks=3, stride=1, bias=False):\n",
    "    return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)\n",
    "\n",
    "# Create a convuolutional layer with convolution and batch norm\n",
    "def conv_layer(ni, nf, ks=3, stride=1, zero_bn=False, act=True):\n",
    "    bn = nn.BatchNorm2d(nf) # get a 2d batch norm from Pytorhc\n",
    "    nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n",
    "    layers = [conv(ni, nf, ks, stride=stride), bn]\n",
    "    if act: layers.append(act_fn) # add in the activation function if act is true\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "# Resblock\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, expansion, ni, nh, stride = 1):\n",
    "        super().__init__()\n",
    "        # ni - number of inputs channels, nf - number of filters\n",
    "        # nh - number of filters in first conv\n",
    "        # expansion is 1 for resnet 18, 34 and 4 for larger networks\n",
    "        nf, ni = nh*expansion, ni*expansion\n",
    "        layers = [conv_layer(ni, nh, 3, stride = stride), # for resnet < 34 2 convs per resblock\n",
    "                 conv_layer(nh, nf, 3, zero_bn = True, act = False)\n",
    "                 ] if expansion == 1 else [ # for RESNET > 34 then 3 convs per block with bottleneck\n",
    "                            conv_layer(ni, nh, 1),\n",
    "                            conv_layer(nh, nh, 3, stride = stride),\n",
    "                            conv_layer(nh, nf, 1, zero_bn = True, act = False)\n",
    "        ]\n",
    "        self.convs = nn.Sequential(*layers) # Creates the conv layers\n",
    "        self.idconv = noop if ni==nf else conv_layer(ni, nf, 1, act = False) # id convolution ()\n",
    "        self.pool = noop if stride== 1 else nn.AvgPool2d(2, ceil_mode = True) # average pool on \n",
    "        \n",
    "    def forward(self, x): \n",
    "        # Forward function adds the convolution part to the id part \n",
    "        #return act_fn(self.convs(x)) + self.idconv(self.pool(x))\n",
    "        return act_fn(self.convs(x) + self.idconv(self.pool(x)))\n",
    "\n",
    "# XResnet\n",
    "class XResNet(nn.Sequential):\n",
    "    @classmethod\n",
    "    def create(cls, expansion, layers, c_in=3, c_out=1000):\n",
    "        nfs = [c_in, (c_in + 1)*8, 64, 64] # number of filters in stem layer (c_in is number of image channels)\n",
    "        stem = [conv_layer(nfs[i], nfs[i+1], stride=2 if i==0 else 1)\n",
    "            for i in range(3)]\n",
    "\n",
    "        nfs = [64//expansion,64,128,256,512]\n",
    "        res_layers = [cls._make_layer(expansion, nfs[i], nfs[i+1],\n",
    "                                      n_blocks=l, stride=1 if i==0 else 2)\n",
    "                  for i,l in enumerate(layers)]\n",
    "        res = cls(\n",
    "        *stem,\n",
    "        nn.MaxPool2d(kernel_size=3, stride = 2, padding = 1), # then a max pooling layer\n",
    "        *res_layers,\n",
    "        nn.AdaptiveAvgPool2d(1), Flatten(), \n",
    "        nn.Linear(nfs[-1]*expansion, c_out)\n",
    "        )\n",
    "        init_cnn(res)\n",
    "        return res\n",
    "        \n",
    "    @staticmethod\n",
    "    def _make_layer(expansion, ni, nf, n_blocks, stride): # returns a resblock\n",
    "        return nn.Sequential(\n",
    "        *[ResBlock(expansion, ni if i==0 else nf, nf, stride if i==0 else 1)\n",
    "         for i in range(n_blocks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xresnet18 (**kwargs): return XResNet.create(1, [2, 2,  2, 2], **kwargs)\n",
    "def xresnet34 (**kwargs): return XResNet.create(1, [3, 4,  6, 3], **kwargs)\n",
    "def xresnet50 (**kwargs): return XResNet.create(4, [3, 4,  6, 3], **kwargs)\n",
    "model = xresnet18(c_in = 1, c_out = 2)\n",
    "#model34 = xresnet34(c_in = 1, c_out = 2)\n",
    "#model50 = xresnet50(c_in = 1, c_out = 2)\n",
    "model.load_state_dict(torch.load(\"/workspace/myFile/Output/17052023/best_model_params.pt\"))\n",
    "model = model.to(device)\n",
    "\n",
    "# Label smoothing cross entropy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def reduce_loss(loss, reduction='mean'):\n",
    "    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, ε:float=0.1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.ε,self.reduction = ε,reduction\n",
    "    \n",
    "    def forward(self, output, target):\n",
    "        c = output.size()[-1]\n",
    "        log_preds = F.log_softmax(output, dim=-1)\n",
    "        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n",
    "        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n",
    "        return lin_comb(loss/c, nll, self.ε)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions that shows the image, true class, predicted class and degree of prediction\n",
    "\n",
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    '''\n",
    "    output = net(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            labels[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            labels[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig\n",
    "\n",
    "def matplotlib_imshow(img, one_channel=True):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_calc(data_loader_test, classes, model_ft):       \n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for inputs, labels, imNo in data_loader_test:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                output = model_ft(inputs) # Feed Network\n",
    "\n",
    "                output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "                y_pred.extend(output) # Save Prediction\n",
    "                \n",
    "                labels = labels.data.cpu().numpy()\n",
    "                y_true.extend(labels) # Save Truth\n",
    "\n",
    "        # constant for classes\n",
    "        # classes = ('Alive', 'Dead')\n",
    "\n",
    "        # Build confusion matrix\n",
    "        cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "        return cf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5 #NUMBER OF K FOLDS\n",
    "epoch=range(5) #NUMBER OF EPOCHS\n",
    "\n",
    "torch.manual_seed(10)\n",
    "imIdx = torch.randperm(1).tolist()\n",
    "\n",
    "## Create dataset\n",
    "dataSetTrain = DNADataset(imDr, get_transform(train = True), labels, imDx=imIdx)\n",
    "dataSetTest = DNADataset(imDr, get_transform(train = False), labels, imDx=imIdx)\n",
    "\n",
    "data_loader_train=[]\n",
    "data_loader_test=[]\n",
    "\n",
    "# ## Create dataloaders\n",
    "# Get subset\n",
    "torch.manual_seed(10)\n",
    "indices = torch.randperm(len(dataSetTrain)).tolist()\n",
    "\n",
    "noTrain = int(len(dataSetTrain)*0.7)\n",
    "train=[]\n",
    "test=[]\n",
    "for epoch in epoch:\n",
    "    indices = torch.randperm(len(dataSetTrain)).tolist()\n",
    "    TotalSet=list(range(len(dataSetTrain)))\n",
    "    Kfifth = len(dataSetTrain)//k\n",
    "    split= epoch%k\n",
    "    train.append(TotalSet[(split+1)*Kfifth:]+TotalSet[:Kfifth*split])\n",
    "\n",
    "    indices = torch.randperm(len(dataSetTest)).tolist()\n",
    "    TotalSet=list(range(len(dataSetTrain)))\n",
    "    test.append(TotalSet[split*Kfifth:(split+1)*Kfifth])\n",
    "\n",
    "#dataset_train = torch.utils.data.Subset(dataSetTrain, indices[-noTrain:])\n",
    "\n",
    "#dataset_test = torch.utils.data.Subset(dataSetTest, indices[:-noTrain])\n",
    "#len(indices), len(indices[:-50]), len(indices[-50:]), 50/191, type(dataset_test)\n",
    "\n",
    "dataset_sizes = {'train': len(train[0]), 'val': len(test[0])}\n",
    "\n",
    "#dataset_train = torch.utils.data.Subset(dataSetTrain, train)\n",
    "#data_loader_train.append(dataset_train)\n",
    "#dataset_testo = torch.utils.data.Subset(dataSetTest, test)\n",
    "#data_loader_test.append(dataset_testo)\n",
    "# define training and validation data loaders\n",
    "\n",
    "\n",
    "# Collate function (gathers together the outputs)\n",
    "# def collate_fn(batch):\n",
    "#     return tuple(zip(*batch))\n",
    "\n",
    "#len(indices[-noTrain:]), dataset_sizes\n",
    "#dataset_test[0][1], dataset_test[3][1], dataset_test[-1][1], dataSetTrain.imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights= [0.924865,0.075135] #1-(#inclass/ #intotal )\n",
    "class_weights = torch.Tensor(class_weights)\n",
    "class_weights=class_weights.to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight = class_weights) \n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model.parameters(), weight_decay=1e-2) # standard ADAM optimiser\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n",
      "val Loss: 6.2194 Acc: 0.5173\n",
      "proper accuracy=\n",
      "tensor(0.0347, device='cuda:0')\n",
      "damaged accuracy=\n",
      "tensor(1., device='cuda:0')\n",
      "undamaged accuracy=\n",
      "tensor(0.0346, device='cuda:0')\n",
      "[[    1     0]\n",
      " [29098  1044]]\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "val Loss: 4.3123 Acc: 0.4233\n",
      "proper accuracy=\n",
      "tensor(0.1196, device='cuda:0')\n",
      "damaged accuracy=\n",
      "tensor(0.7273, device='cuda:0')\n",
      "undamaged accuracy=\n",
      "tensor(0.1194, device='cuda:0')\n",
      "[[    8     3]\n",
      " [26534  3598]]\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "val Loss: 4.6033 Acc: 0.4283\n",
      "proper accuracy=\n",
      "tensor(0.1901, device='cuda:0')\n",
      "damaged accuracy=\n",
      "tensor(0.6667, device='cuda:0')\n",
      "undamaged accuracy=\n",
      "tensor(0.1900, device='cuda:0')\n",
      "[[    4     2]\n",
      " [24411  5726]]\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "val Loss: 3.7689 Acc: 0.4971\n",
      "proper accuracy=\n",
      "tensor(0.2800, device='cuda:0')\n",
      "damaged accuracy=\n",
      "tensor(0.7143, device='cuda:0')\n",
      "undamaged accuracy=\n",
      "tensor(0.2799, device='cuda:0')\n",
      "[[    5     2]\n",
      " [21702  8434]]\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "val Loss: 5.5586 Acc: 0.3334\n",
      "proper accuracy=\n",
      "tensor(0.0420, device='cuda:0')\n",
      "damaged accuracy=\n",
      "tensor(0.6250, device='cuda:0')\n",
      "undamaged accuracy=\n",
      "tensor(0.0418, device='cuda:0')\n",
      "[[    5     3]\n",
      " [28874  1261]]\n",
      "\n",
      "Training complete in 2m 42s\n",
      "Best val Acc: 0.517318\n"
     ]
    }
   ],
   "source": [
    "model_ft, con_mat = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, \n",
    "                                    train,test,num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cf_matrix = confusion_matrix_calc(data_loader_test, labels, model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoIAAAGfCAYAAADLULPNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAj00lEQVR4nO3deZhcZZX48e9JE7ZhkX1LWIWRfQu7g0FAQdkcEEEQYfyZQQUVRMeFYVPHwRmZH44IRnRARQEdlQhBEBQRZUlYBAmLIWwJCfsaBJLuM3/U7dhpSXfl0tXVde/3w3Ofrnvr1q1TeZLi9Dn3fd/ITCRJklQ/o9odgCRJktrDRFCSJKmmTAQlSZJqykRQkiSppkwEJUmSaspEUJIkqaZMBCVJkjpARHw3Ip6IiD8t4vmIiK9HxPSIuDMithvsmiaCkiRJneECYJ8Bnt8X2LjYJgDnDnZBE0FJkqQOkJnXA88McMqBwPey4SbgTRGx1kDXXGIoA3w9rz001aVLJDVl1c0PbXcIkjrEC3NnRLtjmPfUjCHLcZZcbaN/plHF6zUxMycu5mXWAR7tsz+zODZ7US9oeSIoSZKkgRVJ3+Imfm+YiaAkSVIZPd3tjqC/WcDYPvtjimOL5D2CkiRJZWTP0G1DYxJwVDF6eGfg+cxcZFsYrAhKkiR1hIj4ETAeWDUiZgKnAqMBMvM8YDLwLmA68DJwzGDXNBGUJEkqo2fIKnlNyczDB3k+gY8tzjVNBCVJkkrIoWvpto33CEqSJNWUFUFJkqQyhrk13AomgpIkSWXYGpYkSVKnsiIoSZJUxsibUHqxmQhKkiSVYWtYkiRJncqKoCRJUhmOGpYkSaonJ5SWJElSx7IiKEmSVIatYUmSpJqyNSxJkqROZUVQkiSpDCeUliRJqilbw5IkSepUVgQlSZLKcNSwJElSTdkaliRJUqeyIihJklSGrWFJkqR6yuz86WNsDUuSJNWUFUFJkqQyKjBYxERQkiSpDO8RlCRJqqkKVAS9R1CSJKmmrAhKkiSV0dP5o4ZNBCVJksqwNSxJkqROZUVQkiSpDEcNS5Ik1ZStYUmSJHUqK4KSJEll2BqWJEmqqQokgraGJUmSasqKoCRJUgmZTigtSZJUT7aGJUmS1KmsCEqSJJVRgXkETQQlSZLKsDUsSZKkTmVFUJIkqQxbw5IkSTVla1iSJEmdyoqgJElSGbaGJUmSasrWsCRJkjqVFUFJkqQyKlARNBGUJEkqowL3CNoaliRJqikrgpIkSWXYGpYkSaopW8OSJEnqVFYEJUmSyrA1LEmSVFO2hiVJktSprAhKkiSVYWtYkiSppiqQCNoaliRJqikrgpIkSWVktjuCN8xEUJIkqQxbw5IkSepUA1YEI+JFYJF1z8xcYcgjkiRJ6gQVqAgOmAhm5vIAEfFFYDbwfSCAI4C1Wh6dJEnSSFWjCaUPyMxvZuaLmflCZp4LHNjKwCRJktRazSaCcyPiiIjoiohREXEEMLeVgUmSJI1oPT1Dt7VJs4ng+4FDgceL7b3FMUmSpHrKHLqtCRGxT0TcFxHTI+Kzr/P8uhHxm4i4PSLujIh3DXbNpqaPycyHsBUsSZLUFhHRBZwD7A3MBKZExKTMnNbntJOBSzPz3IjYDJgMrD/QdZuqCEbEJhFxbUT8qdjfKiJOLvE5JEmSqmF4W8M7AtMzc0ZmvgZczN8W6RLondFlReCxwS7abGv428DngHkAmXkncFiTr5UkSaqeIUwEI2JCREzts03o927rAI/22Z9ZHOvrNODIiJhJoxp4/GAfodmVRZbNzFsiou+x+U2+VpIkSQPIzInAxDd4mcOBCzLzaxGxC/D9iNgic9Hz3DSbCD4VERtRTC4dEYfQmFdQkiSpnoZ3HsFZwNg++2OKY319CNgHIDNvjIilgVWBJxZ10WYTwY/RyFLfEhGzgAeBI5t8rSRJUuVkT3OjfYfIFGDjiNiARgJ4GH87g8sjwJ7ABRGxKbA08ORAF2121PAMYK+I+DtgVGa+uJjBS5IkqaTMnB8RxwFXAV3AdzPz7og4A5iamZOATwHfjogTaHRxj84ceG6aphLBiDix3z7A88CtmXnH4n4YSZKkjjfME0Fn5mQag0D6Hjulz+NpwG6Lc81mW8Pjiu0Xxf5+wJ3AsRHx48z86uK8qSRJUserwFrDzSaCY4DtMvMlgIg4FbgC2B24FTARlCRJ6jDNJoKrA6/22Z8HrJGZf4mIVxfxGkmSpOoa3sEiLdFsIngRcHNEXFbs7w/8sBg8Mm3RL5MkSaqoYb5HsBWaHTX8xYj4JbBrcejYzJxaPD6iJZFJkiSNZHVJBAEyc0pEPExjThoiYt3MfKRlkUmSJKmlmp0+5gDga8DaNGanXhe4F9i8daFJkiSNYANP0dcRRjV53heBnYH7M3MDYC/gppZFJUmSNNL19Azd1ibNJoLzMvNpYFREjMrM39CYV1CSJEkdqtl7BJ+LiOWA64GLIuIJYG7rwlInu2HKHznzvO/T3d3DP+47nv/3vgMWev6xx5/klLO+zTPPv8CKyy/HVz7zEdZcbRVuueNuvvqtHyw478FHZ/PVzx/Hnrv6O4dUFXvtvTtnfvUUurpGceGFl/JfXztvoeeXXHJJvvXt/2TbbbfgmWee4+ijjueRR2ax/fZbcfY3/g1orG71lS+fzeW/uBqAu6Zdz0svzaW7u5v587sZ/w8HDvvnUk3VaPqYA4FXgBNojBJeETijVUGpc3V39/Dlcy5g4lc+x5qrrsxhx/8re+y8HRutN2bBOf/57R+y/15v5cC9d+fmO+7m7P+5hK985qPsuM3m/OTcrwDw/Asv8a5jTmTX7bZs10eRNMRGjRrF1846nQP3P4pZs+Zw3e9+zuQrruG+e6cvOOeoDx7Kc8+9wDZbvZ2DD9mP07/4LxzzwY8zbdr9vO2tB9Ld3c0aa67GH266gisnX0t3dzcA7973/Tzz9LPt+miqqwqsLNJUazgz52ZmN7AsjWXmfkBjMWNpIXfd9wDrrr0GY9dandGjl2Df8TvzmxtvXeicGQ/PYqetG+OMdtx6s795HuDqG27hrTtszTJLLzUscUtqvXHjtmbGjId56KFHmTdvHv/7k8t59357L3TOu/fbix9d9L8A/PxnVzJ+fGPWsr/85ZUFSd/SSy1VhXv0pRGhqUQwIv45IubQWF94Ko1l5aYO/CrV0RNPP8Oaq62yYH+NVVfm8acW/i19kw3X5ZrfTwHg2t9PZe7Lr/DcCy8udM4vr7uRd43fpfUBSxo2a629JjNnzl6w/9is2ay91hr9zlljwTnd3d288MKLrLzKSkAjkbx5yi+58ZYr+eTHT16QGGYmP590Ib+94TKOPuawYfo0Eo3W8FBtbdLsYJGTgC0yc/3M3DAzN8jMDRd1ckRMiIipETH1/B/+dGgiVWWcNOEIpt51D+/96OeZetc9rL7qSowa9de/ik8+/Sx/fuhRdh23VRujlDTSTJ36R3baYR/G734QnzrpIyy11JIAvHOvQ9l9twM4+D3/xIf/+QPsutsObY5UdZE9PUO2tUuz9wg+ALzc7EUzcyIwEeC1h6ZawK+R1VdZmTlPPr1g//GnnmGNVVfqd85K/P9TTgDg5b+8wq9uuIUVlvu7Bc9fdf3NvH3XcYxeoun5ziV1gNmPzWHMmLUW7K+9zlo8Nvvxfuc8zpgxa/HYY3Po6upihRWW/5t7/+6/7wFemjuXzTb7e26//S5mF9d46smnuXzS1Ww/bmv+UHQdJA2s2Yrg54A/RMS3IuLrvVsrA1Nn2uLvN+ThWXOYOecJ5s2bz5XX3cT4nbdf6Jxnn3+RnuK3n/MvnsR73jF+oeevvO4PtoWlCrr11jvZcKP1WW+9MYwePZqDD9mPyVdcs9A5k6+4lsOPOBiAg96zL7/97Y0ArLfeGLq6ugAYO3ZtNtlkIx5+ZCbLLrsMyxW/SC677DK8fc+3cs+0+4fxU6nWKtAabrbk8i3g18BdQOcPkVHLLNHVxec/djTHfv5Munt6eM873sab1x/DNy78CZtvsgF77LI9U+6cxtnfvYSIYPst38IXPnb0gtfPmvMkc558hnFbbdq+DyGpJbq7u/n0p07jZ5ddSFfXKL7/vR9z7z1/5gsnf5LbbruLKydfy/cuvISJ55/FHXf+mmeffZ5jPvhxAHbZdRwnnHgs8+bPp6enhxM/eQrPPP0s668/losubkxBs0RXFz++dBLX/Or6dn5M1UkFRg1HNjH0KiJuz8xty7yBrWFJzVp180PbHYKkDvHC3BnR7hjmfunIIctx/u7kH7Tl8zRbEbwyIibQmDrm1d6DmflMS6KSJEka6Wo0ofThxc/P9TmWwCJHDkuSJFVaG0f7DpWmEsHM3KDVgUiSJGl4NT0/R0RsAWwGLN17LDO/14qgJEmSRry6tIYj4lRgPI1EcDKwL3ADYCIoSZLqqQKjhpudR/AQYE9gTmYeA2wNrNiyqCRJktRyzbaG/5KZPRExPyJWAJ4AxrYwLkmSpJGtLq1hYGpEvAn4NnAr8BJwY6uCkiRJGunauUbwUGl21PBHi4fnRcQvgRUy887WhSVJkqRWGzARjIjtBnouM28b+pAkSZI6QA1aw18rfi4NjAP+CASwFTAV2KV1oUmSJI1gFUgEBxw1nJl7ZOYewGxgu8wcl5nbA9sCs4YjQEmSJLVGs4NF/j4z7+rdycw/RcSmLYpJkiRp5KvAPILNJoJ3RsT5wA+K/SMAB4tIkqT6qkBruNlE8BjgI8Aniv3rgXNbEpEkSZKGRbPTx7wC/FexSZIk1V7WpSIYEbsBpwHr9X1NZm7YmrAkSZJGuLokgsB3gBNorCrS3bpwJEmSNFyaTQSfz8wrWxqJJElSJ6nLEnPAbyLiP4CfAq/2HnRlEUmSVFs1ag3vVPzcvvgZQAJvH/KIJEmSNCwGW2v4xOLh5cXPBJ4EbsjMB1sZmCRJ0ohWgYrggEvMAcsX23LFtjyNNYevjIjDWhybJEnSiJWZQ7a1y4AVwcw8/fWOR8TKwDXAxa0ISpIkSa3X7D2CC8nMZyIihjoYSZKkjlGB1nCpRDAi9gCeHeJYJEmSOkfVE8GIuIvGAJG+VgYeA45qVVCSJElqvcEqgvv120/g6cyc26J4JEmSOkLl1xrOzIeHKxBJkqSOUoFEcLDpYyRJklRRpQaLSJIk1V7nLzVsIihJklRGFe4RtDUsSZJUU1YEJUmSyqhARdBEUJIkqYwK3CNoa1iSJKmmrAhKkiSVUIXBIiaCkiRJZdgaliRJUqeyIihJklSCrWFJkqS6qkBr2ERQkiSphKxAIug9gpIkSTVlRVCSJKmMClQETQQlSZJKsDUsSZKkjmVFUJIkqYwKVARNBCVJkkqwNSxJkqSOZSIoSZJUQvYM3daMiNgnIu6LiOkR8dlFnHNoREyLiLsj4oeDXdPWsCRJUgnD2RqOiC7gHGBvYCYwJSImZea0PudsDHwO2C0zn42I1Qe7rhVBSZKkkW9HYHpmzsjM14CLgQP7nfNh4JzMfBYgM58Y7KImgpIkSWVkDNkWERMiYmqfbUK/d1sHeLTP/sziWF+bAJtExO8j4qaI2Gewj2BrWJIkqYShbA1n5kRg4hu8zBLAxsB4YAxwfURsmZnPLeoFVgQlSZJGvlnA2D77Y4pjfc0EJmXmvMx8ELifRmK4SCaCkiRJJWRPDNnWhCnAxhGxQUQsCRwGTOp3zs9pVAOJiFVptIpnDHRRW8OSJEklDOeo4cycHxHHAVcBXcB3M/PuiDgDmJqZk4rn3hER04Bu4NOZ+fRA1zURlCRJ6gCZORmY3O/YKX0eJ3BisTXFRFCSJKmEzKZauiOaiaAkSVIJrjUsSZKkjmVFUJIkqYQmR/uOaCaCkiRJJWS2O4I3ztawJElSTVkRlCRJKsHWsCRJUk1VIRG0NSxJklRTVgQlSZJKqMJgERNBSZKkEmwNS5IkqWNZEZQkSSrBtYYlSZJqyrWGJUmS1LGsCEqSJJXQY2tYkiSpnqpwj6CtYUmSpJqyIihJklRCFeYRNBGUJEkqoQori9galiRJqikrgpIkSSXYGpYkSaqpKkwfY2tYkiSppqwISpIklVCFeQRNBCVJkkpw1LAkSZI6lhVBSZKkEqowWMREUJIkqYQq3CNoa1iSJKmmrAhKkiSVUIXBIiaCkiRJJVThHkFbw5IkSTXV8opgLLdyq99CUkW8PO/VdocgSU2rwmARW8OSJEkl2BqWJElSx7IiKEmSVEIFBg2bCEqSJJVRhdawiaAkSVIJVRgs4j2CkiRJNWVFUJIkqYSedgcwBEwEJUmSSkhsDUuSJKlDWRGUJEkqoacC88eYCEqSJJXQY2tYkiRJncqKoCRJUglVGCxiIihJklRCFaaPsTUsSZJUU1YEJUmSSrA1LEmSVFO2hiVJktSxrAhKkiSVUIWKoImgJElSCVW4R9DWsCRJUk1ZEZQkSSqhp/MLgiaCkiRJZbjWsCRJkjqWFUFJkqQSst0BDAETQUmSpBKqMH2MrWFJkqSasiIoSZJUQk90/mARE0FJkqQSqnCPoK1hSZKkmrIiKEmSVEIVBouYCEqSJJVQhZVFbA1LkiTVlBVBSZKkElxiTpIkqaZyCLdmRMQ+EXFfREyPiM8OcN7BEZERMW6wa5oISpIkjXAR0QWcA+wLbAYcHhGbvc55ywOfAG5u5romgpIkSSX0xNBtTdgRmJ6ZMzLzNeBi4MDXOe+LwJnAK81c1ERQkiSphJ4h3CJiQkRM7bNN6Pd26wCP9tmfWRxbICK2A8Zm5hXNfgYHi0iSJLVZZk4EJpZ9fUSMAs4Cjl6c15kISpIklTDMS8zNAsb22R9THOu1PLAFcF001kBeE5gUEQdk5tRFXdREUJIkqYRhnlB6CrBxRGxAIwE8DHh/75OZ+Tywau9+RFwHnDRQEgjeIyhJkjTiZeZ84DjgKuAe4NLMvDsizoiIA8pe14qgJElSCcO91nBmTgYm9zt2yiLOHd/MNU0EJUmSShjuRLAVbA1LkiTVlBVBSZKkErLzlxo2EZQkSSrD1rAkSZI6lhVBSZKkEqpQETQRlCRJKmGYVxZpCVvDkiRJNWVFUJIkqYRhXmKuJUwEJUmSSqjCPYK2hiVJkmrKiqAkSVIJVagImghKkiSV4KhhSZIkdSwrgpIkSSU4aliSJKmmvEdQkiSpprxHUJIkSR3LiqAkSVIJPRWoCZoISpIklVCFewRtDUuSJNWUFUFJkqQSOr8xbCIoSZJUiq1hSZIkdSwrgpIkSSW4sogkSVJNVWH6GFvDkiRJNWVFUJIkqYTOrweaCEqSJJXiqGFJkiR1rAErghHx3wxQ+czMjw95RJIkSR2gDoNFpgK3AksD2wF/LrZtgCVbGpkkSdIIlkO4tcuAFcHMvBAgIj4CvDUz5xf75wG/a314kiRJapVmB4usBKwAPFPsL1cckyRJqqUqDBZpNhH8d+D2iPgNEMDuwGmtCkqSJGmkq8I9gk0lgpn5PxFxJbBTcehfMnNO68KSJElSqzU1fUxEBLAXsHVmXgYsGRE7tjQySZKkEawKg0WanUfwm8AuwOHF/ovAOS2JSJIkqQP0DOHWLs3eI7hTZm4XEbcDZOazEeH0MZIkSR2s2URwXkR0UVQvI2I1qjFYRpIkqZSsy2AR4OvAz4DVI+LLwCHAyS2LSpIkaYSrQkWs2VHDF0XErcCeNKaPOSgz72lpZJIkSWqpphLBiFgZeAL4UZ9jozNzXqsCkyRJGslqM48gcBswFniWRkXwTcCciHgc+HBm3tqa8CRJkkamzk8Dm58+5lfAuzJz1cxcBdgXuBz4KI2pZSRJktRhmk0Ed87Mq3p3MvNqYJfMvAlYqiWRSZIkjWA95JBt7dJsa3h2RPwLcHGx/z7g8WJKmSoMmpEkSVosVUiAmq0Ivh8YA/y82NYtjnUBh7YiMFXTyf92Fru/+zAOOvLYdociaYR75zvGc/efrufeaTfwmU9/rN3hSJXUVCKYmU9l5vGZuW2xHZeZT2bma5k5vdVBqjoOetfenHfWl9odhqQRbtSoUXz97C+z3/5HsuXWe/C+9x3Epptu3O6wpIXkEP7XLs1OH7Ma8Blgc2Dp3uOZ+fYWxaWKGrfNlsya/Xi7w5A0wu24w7Y88MBDPPjgIwBceullHLD/O7nnnj+3OTLpr+rUGr4IuBfYADgdeAiY0qKYJEk1t/Y6a/LozMcW7M+cNZu1116zjRFJ1dRsIrhKZn4HmJeZv83MfwIWWQ2MiAkRMTUipp7/vR8t6jRJkqSOVZvWMNC7gsjsiHg38Biw8qJOzsyJwESAeU/NqMJ8i5KkYfTYrDmMHbP2gv0x66zFY4/NaWNE0t+qU2v4SxGxIvAp4CTgfOCElkUlSaq1KVPv4M1v3oD11x/L6NGjOfTQA/nF5Ve3OyypcpqqCGbm5cXD54E9WheOqu7Tp/47U26/k+eee4E9DzqSj37oAxy8/zvbHZakEaa7u5tPfPJkJl/xQ7pGjeKCCy9h2rT72x2WtJCe7PymZ2QTHyIiNgCOB9anT/KYmQcM9lpbw5Katcza/9DuECR1iPmvzYp2x3Dkev84ZDnODx7+aVs+T7P3CP4c+A7wC6rREpckSaq9ZhPBVzLz6y2NRJIkqYO0c43godJsInh2RJwKXA282nswM29rSVSSJEkjXDunfRkqzSaCWwIfoDF3YG9rOBlgLkFJkiSNbM0mgu8FNszM11oZjCRJUqeowqCJZhPBPwFvAp5oXSiSJEmdo073CL4JuDciprDwPYKDTh8jSZKkkanZRPDUlkYhSZLUYWozWCQzf9vqQCRJkjpJFe4RbGqt4YjYOSKmRMRLEfFaRHRHxAutDk6SJEmt02xr+BvAYcCPgXHAUcAmrQpKkiRppGtmmd6RrqmKIEBmTge6MrM7M/8H2Kd1YUmSJI1sPeSQbc2IiH0i4r6ImB4Rn32d50+MiGkRcWdEXBsR6w12zWYTwZcjYkngjoj4akScsBivlSRJ0hsQEV3AOcC+wGbA4RGxWb/TbgfGZeZWwE+Arw523WaTuQ8U5x4HzAXGAgc3+VpJkqTK6RnCrQk7AtMzc0axwMfFwIF9T8jM32Tmy8XuTcCYwS7a7KjhhyNiteLx6c3FK0mSVF1DOX1MREwAJvQ5NDEzJ/bZXwd4tM/+TGCnAS75IeDKwd53wEQwIoLGHILH0agIRkTMB/47M88Y7OKSJElVNZQrixRJ38RBT2xCRBxJY3Dv2wY7d7DW8AnAbsAOmblyZq5EI/vcrbhPUJIkSa03i8ateb3GFMcWEhF7AV8ADsjMV/s/399gieAHgMMz88HeA5k5AziSxhQykiRJtZSZQ7Y1YQqwcURsUAzgPQyY1PeEiNgW+BaNJPCJZi462D2CozPzqdf54E9GxOhm3kCSJKmKhnNlkcycHxHHAVcBXcB3M/PuiDgDmJqZk4D/AJYDfty4u49HMvOAga47WCL4WsnnJEmSNIQyczIwud+xU/o83mtxrzlYIrj1IpaSC2DpxX0zSZKkqhjKUcPtMmAimJldwxWIJElSJxnKUcPt4uogkiRJNdXUhNKSJElaWJOjfUc0E0FJkqQSbA1LkiSpY1kRlCRJKqHyo4YlSZL0+noqcI+grWFJkqSasiIoSZJUQufXA00EJUmSSnHUsCRJkjqWFUFJkqQSqlARNBGUJEkqoQori9galiRJqikrgpIkSSXYGpYkSaqpKqwsYmtYkiSppqwISpIklVCFwSImgpIkSSVU4R5BW8OSJEk1ZUVQkiSpBFvDkiRJNWVrWJIkSR3LiqAkSVIJVZhH0ERQkiSphJ4K3CNoa1iSJKmmrAhKkiSVYGtYkiSppmwNS5IkqWNZEZQkSSrB1rAkSVJN2RqWJElSx7IiKEmSVIKtYUmSpJqyNSxJkqSOZUVQkiSpBFvDkiRJNZXZ0+4Q3jBbw5IkSTVlRVCSJKmEHlvDkiRJ9ZSOGpYkSVKnsiIoSZJUgq1hSZKkmrI1LEmSpI5lRVCSJKmEKiwxZyIoSZJUQhVWFrE1LEmSVFNWBCVJkkqowmARE0FJkqQSnD5GkiSppqpQEfQeQUmSpJqyIihJklSC08dIkiTVlK1hSZIkdSwrgpIkSSU4aliSJKmmbA1LkiSpY1kRlCRJKsFRw5IkSTWVFbhH0NawJElSTVkRlCRJKsHWsCRJUk05aliSJEkdy4qgJElSCVUYLGIiKEmSVIKtYUmSJHUsE0FJkqQSMnPItmZExD4RcV9ETI+Iz77O80tFxCXF8zdHxPqDXdNEUJIkqYQcwm0wEdEFnAPsC2wGHB4Rm/U77UPAs5n5ZuC/gDMHu66JoCRJ0si3IzA9M2dk5mvAxcCB/c45ELiwePwTYM+IiIEu2vLBIqNX3XDAAFRPETEhMye2Ow6NLPNfm9XuEDQC+X2hkWr+a7OGLMeJiAnAhD6HJvb7e78O8Gif/ZnATv0us+CczJwfEc8DqwBPLep9rQiqXSYMfookAX5fqAYyc2JmjuuzDcsvPyaCkiRJI98sYGyf/THFsdc9JyKWAFYEnh7ooiaCkiRJI98UYOOI2CAilgQOAyb1O2cS8MHi8SHAr3OQIclOKK128X4fSc3y+0K1V9zzdxxwFdAFfDcz746IM4CpmTkJ+A7w/YiYDjxDI1kcUFRhVmxJkiQtPlvDkiRJNWUiKEmSVFMmgiIi1o+IP/U7dlpEnLQY17guIsYNfXRDJyJeancMUlVFRHdE3BERd0fEHyPiUxExov8fExFHR8Q32h2H1E4OFpEkDYW/ZOY2ABGxOvBDYAXg1HYGJWlgI/q3NbVfUek7MyJuiYj7I+IfiuPLRMTFEXFPRPwMWKbPa86NiKlFZeD0PscfioivFFWDqRGxXURcFREPRMSxxTnLRcS1EXFbRNwVEQf2ef2/Fott3xARP+qtWEbERhHxy4i4NSJ+FxFvKY5vEBE3Ftf50jD9kUm1l5lP0JgE+rhoWL/4t3lbse0KEBHjI+K3EXFZRMyIiH+PiCOK75u7ImKj4rz9I+LmiLg9Iq6JiDWK46tFxK+K75rzI+LhiFi1eO7I4jp3RMS3inVaiYhjiu+yW4Dd2vIHJI0gJoJqxhKZuSPwSf762/1HgJczc9Pi2PZ9zv9CZo4DtgLeFhFb9XnukaJq8DvgAhrzHO0M9CaMrwDvycztgD2ArxX/I9kBOBjYmsaC233b0BOB4zNze+Ak4JvF8bOBczNzS2D2G/oTkLRYMnMGjSkuVgeeAPYu/l2/D/h6n1O3Bo4FNgU+AGxSfN+cDxxfnHMDsHNmbktjfdXPFMdPpTFP2uY01lVdFyAiNi3eZ7fi+6YbOCIi1qLxXbMb8FZgs6H/5FJnsTUsgEXNIdR7/KfFz1uB9YvHu1N8mWfmnRFxZ5/XHVqsmbgEsBaNL9ve53snv7wLWC4zXwRejIhXI+JNwFzg3yJid6CHxrqJa9D44r4sM18BXomIX0CjggjsCvy4z7raSxU/d6ORPAJ8Hzhz0D8JSa0wGvhGRGxDIynbpM9zUzJzNkBEPABcXRy/i8Yvg9BYQeGSIpFbEniwOP5W4D0AmfnLiHi2OL4njV9OpxTfC8vQSEZ3Aq7LzCeL97ukXyxS7ZgIChrLz6zU79jK/PXL9tXiZzeD/J2JiA1oVOV2yMxnI+ICYOk+p/Req6fP4979JYAjgNWA7TNzXkQ81O/1/Y0Cnuu9N+l1OFGm1AYRsSGN74wnaFTuHqdR/RtFo/Lfq//3QN/viN7vm/8GzsrMSRExHjhtsLcHLszMz/WL6aDF/BhS5dkaFpn5EjA7It4OEBErA/vQaMcsyvXA+4vzt6DRBobGzeFzgeeL+3j2XcxwVgSeKJLAPYD1iuO/B/aPiKWLKuB+RewvAA9GxHuLWCIitu7zmt5Z1Y9YzDgklRQRqwHnAd8olrdaEZidmT002r9di3nJFfnrmqof7HP898ChxXu+g7/+QnstcEgxaIWIWDki1gNupnG7yioRMRp472J/OKliTATV6yjgXyPiDuDXwOmZ+cAA558LLBcR9wBn0Ggbk5l/BG4H7qUxavD3ixnHRcC4iLiriOne4rpTaLSV7wSupNE2er54zRHAhyLij8DdQO8Ak08AHyuutc5ixiFp8SxTDMy4G7iGRou3997fbwIfLP6NvoXGL4uL4zQat3/cCjzV5/jpwDuiMf3Ve4E5wIuZOQ04Gbi6uG3lV8BaRQv6NOBGGt9N9yz2p5QqxiXm1DEiYrnMfCkilqVRkZyQmbe1Oy5J7RERSwHdxRqsu9AYHLZNm8OSOor3CKqTTIyIzWjcM3ihSaBUe+sCl0Zj4urXgA+3OR6p41gRlCRJqinvEZQkSaopE0FJkqSaMhGUJEmqKRNBSZKkmjIRlCRJqqn/A/3dLPLZsOjEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "con_mat=con_mat.cpu().numpy()\n",
    "df_cm_ratio = pd.DataFrame(con_mat / np.sum(con_mat, axis=1)[:, None], index=[i for i in labels],\n",
    "                     columns=[i for i in labels])\n",
    "plt.figure(figsize=(12, 7))\n",
    "sn.heatmap(df_cm_ratio, annot=True)\n",
    "# plt.save(\"Confusion_matrix_ratio.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_cm_raw = pd.DataFrame(con_mat, index=[i for i in labels],\n",
    "                     columns=[i for i in labels])\n",
    "plt.figure(figsize=(12, 7))\n",
    "sn.heatmap(df_cm_raw, annot=True)\n",
    "# plt.save(\"Confusion_matrix_raw.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29098,  1044],\n",
       "       [    1,     0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/workspace/myFile/Output/11082023/B0115/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/B0115/Damaged_nuclei_2.tiff',\n",
       " '/workspace/myFile/Output/11082023/B0322/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/D0725/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/D0121/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/E0723/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/F0125/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/F0125/Damaged_nuclei_2.tiff',\n",
       " '/workspace/myFile/Output/11082023/F0324/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/C0212/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/B0112/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/E0108/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/B0113/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/B0113/Damaged_nuclei_2.tiff',\n",
       " '/workspace/myFile/Output/11082023/D0722/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/B0210/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/E0225/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/B0310/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/C0719/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/B0105/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/B0801/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/B0309/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/A0121/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/F0124/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/F0925/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/B0925/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/C0305/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/F0103/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/D0922/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/F0121/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/F0922/Damaged_nuclei_1.tiff',\n",
       " '/workspace/myFile/Output/11082023/F0922/Damaged_nuclei_2.tiff',\n",
       " '/workspace/myFile/Output/11082023/D0125/Damaged_nuclei_1.tiff']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weird = glob.glob(\"/workspace/myFile/Output/11082023/*/*.tiff\")\n",
    "Damagednuclei= [x for x in weird if 'Damaged_nuclei_' in x]\n",
    "print(len(Damagednuclei))\n",
    "Damagednuclei"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
