{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To note: Running this on different systems (i.e. local, SCW, server) will result in slight changes needing to the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x7f25e8631640>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from PIL import ImageEnhance\n",
    "import matplotlib.pyplot as plt\n",
    "import imagesize\n",
    "import subprocess\n",
    "sys.path\n",
    "from IPython.core.debugger import set_trace\n",
    "import scipy.ndimage\n",
    "import matplotlib.patches as patches\n",
    "# plt.rcParams['figure.figsize'] = [12,12]\n",
    "# sys.path.append('/workspace/myFile/Mask_RCNN_Tutorial/')\n",
    "from tqdm import tqdm\n",
    "from torch import nn as nn\n",
    "from torch import optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import re\n",
    "import time\n",
    "import copy\n",
    "import pylab\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tempfile import TemporaryDirectory\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import glob\n",
    "# Rather than have a messy notebook with a load of functions we can store them in separate .py files and import them\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "writer = SummaryWriter(\"Experiments/TENSORBOARD\")    # This determines the tensorboard file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNADataset(object):\n",
    "    def __init__(self, root, transforms, labels, imDx = False):\n",
    "        self.root, self.transforms, self.labels = root, transforms, labels\n",
    "    \n",
    "        # load all image files, sorting them to ensure they are aligned\n",
    "        self.imgDir = glob.glob(root+\"*/*.tiff\")\n",
    "        Damagednuclei= [x for x in self.imgDir if 'Damaged_nuclei_' in x]\n",
    "        Undamagednuclei= [x for x in self.imgDir if \"No_damage_nuclei\" in x]\n",
    "        #np.random.shuffle(Undamagednuclei)\n",
    "        #Undamagednuclei=Undamagednuclei[:10000]\n",
    "        self.imgDir= Damagednuclei+Undamagednuclei\n",
    "        size80=[]\n",
    "        for x in self.imgDir:\n",
    "            img = Image.open(x) # Open image\n",
    "            w,h=img.size\n",
    "            if w<=80 and h<=80:\n",
    "                size80.append(x)\n",
    "        self.imgDir=size80              \n",
    "        \n",
    "        self.imgs = sorted(self.imgDir) # list of images\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.imgs[idx]\n",
    "       \n",
    "        # Transform images into tensors\n",
    "        img = Image.open(img_path) # Open image\n",
    "        w,h=img.size\n",
    "        img = np.array(img) # Convert image into an array\n",
    "        img = np.float32(np.divide(img, 2**16)) # Ensure all values are floats\n",
    "        \n",
    "        result=np.zeros((80,80), dtype=np.float32)\n",
    "        x_center = (80 - w) // 2\n",
    "        y_center = (80 - h) // 2 # copy img image into center of result image\n",
    "        result[y_center:y_center+h, x_center:x_center+w] = img\n",
    "        img = result\n",
    "        \n",
    "        targetlab=\"\"\n",
    "        if img_path.find('No_damage_nuclei') != -1:\n",
    "            targetlab= 'Undamaged'\n",
    "        if img_path.find('Damaged_nuclei_') != -1:\n",
    "            targetlab= 'Damaged'  # Find labels corresponding to image\n",
    "        target = self.labels.index(targetlab) # Get the label and assign to a value\n",
    "        \n",
    "        # Convert label to tensor\n",
    "        #torch.to\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "#             #print('In the transforms')\n",
    "        imNo = idx\n",
    "        return img, target, imNo\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imDr = \"/workspace/myFile/Output/17052023/\"  # Image patches directory\n",
    "\n",
    "labels = ['Damaged', 'Undamaged']  # Your labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data augmentation\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "\n",
    "    transforms.append(T.ToTensor())\n",
    "    #transforms.append(T.Normalize([0.0019368887995516483], [0.00672996630111016]))\n",
    "    #transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    \n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(p=1))\n",
    "        transforms.append(T.RandomVerticalFlip(p=1))\n",
    "    \n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, train, test, num_epochs=25):\n",
    "    since = time.time()\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "            dataset_train = torch.utils.data.Subset(dataSetTrain, train[epoch])\n",
    "            dataset_test = torch.utils.data.Subset(dataSetTest, test[epoch])\n",
    "            data_loader_train = torch.utils.data.DataLoader(\n",
    "                dataset_train, batch_size=128, shuffle=True, num_workers=0)\n",
    "\n",
    "            data_loader_test = torch.utils.data.DataLoader(\n",
    "                dataset_test, batch_size=128, shuffle=False, num_workers=0)\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                    dataloaders = data_loader_train\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "                    dataloaders = data_loader_test\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "                damaged_len=0\n",
    "                damaged_corrects = 0\n",
    "                undamaged_len=0\n",
    "                undamaged_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels, imNo in dataloaders:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)    # Loss\n",
    "                    nada=torch.tensor(np.zeros(len(labels))).to(device)\n",
    "                    uno=nada+1\n",
    "                    falseneg=preds+1\n",
    "                    falsepos=preds-1\n",
    "                    FPplusTN=torch.sum(labels==nada)\n",
    "                    TPplusFN=torch.sum(labels==uno)\n",
    "                    FN=torch.sum(labels==falseneg)\n",
    "                    FP=torch.sum(labels==falsepos)\n",
    "                    TN=FPplusTN-FP\n",
    "                    TP=TPplusFN-FN\n",
    "                    running_corrects += torch.sum(preds == labels.data) # Accuracy\n",
    "                    damaged_len+=TPplusFN\n",
    "                    damaged_corrects += TP\n",
    "                    undamaged_len+=FPplusTN\n",
    "                    undamaged_corrects += TN\n",
    "                damaged_acc = damaged_corrects / damaged_len\n",
    "                undamaged_acc = undamaged_corrects/undamaged_len\n",
    "                epoch_acc= (damaged_acc+undamaged_acc)/2\n",
    "                proper_acc=(damaged_corrects+undamaged_corrects)/(damaged_len+undamaged_len)\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]    # Loss metric per epoch\n",
    "                #epoch_acc = running_corrects.double() / dataset_sizes[phase]    # Accuracy metric per epoch\n",
    "\n",
    "                if phase == \"train\":    # This is the tensorboard code that writes accuracy and loss metrics\n",
    "                    writer.add_scalar(\"Train/Accuracy\", epoch_acc, epoch)\n",
    "                    writer.add_scalar(\"Train/Loss\", epoch_loss, epoch)\n",
    "                else:\n",
    "                    writer.add_scalar(\"Validation/Accuracy\", epoch_acc, epoch)\n",
    "                    writer.add_scalar(\"Validation/Loss\", epoch_loss, epoch)\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "                print(\"proper accuracy=\")\n",
    "                print(proper_acc)\n",
    "                print(\"damaged accuracy=\")\n",
    "                print(damaged_acc)\n",
    "                print(\"undamaged accuracy=\")\n",
    "                print(undamaged_acc)\n",
    "                con_matter= torch.tensor([[damaged_corrects, damaged_len-damaged_corrects],[undamaged_len-undamaged_corrects,undamaged_corrects]])\n",
    "                con_matter=con_matter.cpu().numpy()\n",
    "                print(con_matter)\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and epoch_acc >= best_acc: \n",
    "                    # This compares validation accuracy to previous bests and adjusts model weights accordingly\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "                    con_mat= torch.tensor([[damaged_corrects, damaged_len-damaged_corrects],[undamaged_len-undamaged_corrects,undamaged_corrects]])\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since  # Nice way to measure training time but info also stored (indirectly) by tensorboard\n",
    "        print(\n",
    "            f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "        #labels= labels.cpu().numpy()\n",
    "        #preds=preds.cpu().numpy()\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    writer.close()\n",
    "    return model, con_mat   # We want to return the model because its the model, also confusion matrix for later analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is your RESNET\n",
    "# Initialize CNN with kaiming\n",
    "def init_cnn(m):\n",
    "    # Set the weights of the RESNET\n",
    "    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)\n",
    "    if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight)\n",
    "    for l in m.children(): init_cnn(l)\n",
    "\n",
    "\n",
    "# noop function for returning nothing\n",
    "def noop(x): return x\n",
    "# activation function(RELU)\n",
    "act_fn = nn.ReLU(inplace=True)\n",
    "\n",
    "# Flatten\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): return x.view(x.size(0), -1)\n",
    "\n",
    "# Make a convolution\n",
    "def conv(ni, nf, ks=3, stride=1, bias=False):\n",
    "    return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)\n",
    "\n",
    "# Create a convuolutional layer with convolution and batch norm\n",
    "def conv_layer(ni, nf, ks=3, stride=1, zero_bn=False, act=True):\n",
    "    bn = nn.BatchNorm2d(nf) # get a 2d batch norm from Pytorhc\n",
    "    nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n",
    "    layers = [conv(ni, nf, ks, stride=stride), bn]\n",
    "    if act: layers.append(act_fn) # add in the activation function if act is true\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "# Resblock\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, expansion, ni, nh, stride = 1):\n",
    "        super().__init__()\n",
    "        # ni - number of inputs channels, nf - number of filters\n",
    "        # nh - number of filters in first conv\n",
    "        # expansion is 1 for resnet 18, 34 and 4 for larger networks\n",
    "        nf, ni = nh*expansion, ni*expansion\n",
    "        layers = [conv_layer(ni, nh, 3, stride = stride), # for resnet < 34 2 convs per resblock\n",
    "                 conv_layer(nh, nf, 3, zero_bn = True, act = False)\n",
    "                 ] if expansion == 1 else [ # for RESNET > 34 then 3 convs per block with bottleneck\n",
    "                            conv_layer(ni, nh, 1),\n",
    "                            conv_layer(nh, nh, 3, stride = stride),\n",
    "                            conv_layer(nh, nf, 1, zero_bn = True, act = False)\n",
    "        ]\n",
    "        self.convs = nn.Sequential(*layers) # Creates the conv layers\n",
    "        self.idconv = noop if ni==nf else conv_layer(ni, nf, 1, act = False) # id convolution ()\n",
    "        self.pool = noop if stride== 1 else nn.AvgPool2d(2, ceil_mode = True) # average pool on \n",
    "        \n",
    "    def forward(self, x): \n",
    "        # Forward function adds the convolution part to the id part \n",
    "        #return act_fn(self.convs(x)) + self.idconv(self.pool(x))\n",
    "        return act_fn(self.convs(x) + self.idconv(self.pool(x)))\n",
    "\n",
    "# XResnet\n",
    "class XResNet(nn.Sequential):\n",
    "    @classmethod\n",
    "    def create(cls, expansion, layers, c_in=3, c_out=1000):\n",
    "        nfs = [c_in, (c_in + 1)*8, 64, 64] # number of filters in stem layer (c_in is number of image channels)\n",
    "        stem = [conv_layer(nfs[i], nfs[i+1], stride=2 if i==0 else 1)\n",
    "            for i in range(3)]\n",
    "\n",
    "        nfs = [64//expansion,64,128,256,512]\n",
    "        res_layers = [cls._make_layer(expansion, nfs[i], nfs[i+1],\n",
    "                                      n_blocks=l, stride=1 if i==0 else 2)\n",
    "                  for i,l in enumerate(layers)]\n",
    "        res = cls(\n",
    "        *stem,\n",
    "        nn.MaxPool2d(kernel_size=3, stride = 2, padding = 1), # then a max pooling layer\n",
    "        *res_layers,\n",
    "        nn.AdaptiveAvgPool2d(1), Flatten(), \n",
    "        nn.Linear(nfs[-1]*expansion, c_out)\n",
    "        )\n",
    "        init_cnn(res)\n",
    "        return res\n",
    "        \n",
    "    @staticmethod\n",
    "    def _make_layer(expansion, ni, nf, n_blocks, stride): # returns a resblock\n",
    "        return nn.Sequential(\n",
    "        *[ResBlock(expansion, ni if i==0 else nf, nf, stride if i==0 else 1)\n",
    "         for i in range(n_blocks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xresnet18 (**kwargs): return XResNet.create(1, [2, 2,  2, 2], **kwargs)\n",
    "def xresnet34 (**kwargs): return XResNet.create(1, [3, 4,  6, 3], **kwargs)\n",
    "def xresnet50 (**kwargs): return XResNet.create(4, [3, 4,  6, 3], **kwargs)\n",
    "model = xresnet18(c_in = 1, c_out = 2)\n",
    "#model34 = xresnet34(c_in = 1, c_out = 2)\n",
    "#model50 = xresnet50(c_in = 1, c_out = 2)\n",
    "model.load_state_dict(torch.load(\"/workspace/myFile/Output/17052023/best_model_params.pt\"))\n",
    "model = model.to(device)\n",
    "\n",
    "# Label smoothing cross entropy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def reduce_loss(loss, reduction='mean'):\n",
    "    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, ε:float=0.1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.ε,self.reduction = ε,reduction\n",
    "    \n",
    "    def forward(self, output, target):\n",
    "        c = output.size()[-1]\n",
    "        log_preds = F.log_softmax(output, dim=-1)\n",
    "        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n",
    "        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n",
    "        return lin_comb(loss/c, nll, self.ε)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions that shows the image, true class, predicted class and degree of prediction\n",
    "\n",
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    '''\n",
    "    output = net(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            labels[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            labels[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig\n",
    "\n",
    "def matplotlib_imshow(img, one_channel=True):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_calc(data_loader_test, classes, model_ft):       \n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for inputs, labels, imNo in data_loader_test:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                output = model_ft(inputs) # Feed Network\n",
    "\n",
    "                output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "                y_pred.extend(output) # Save Prediction\n",
    "                \n",
    "                labels = labels.data.cpu().numpy()\n",
    "                y_true.extend(labels) # Save Truth\n",
    "\n",
    "        # constant for classes\n",
    "        # classes = ('Alive', 'Dead')\n",
    "\n",
    "        # Build confusion matrix\n",
    "        cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "        return cf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5 #NUMBER OF K FOLDS\n",
    "epoch=range(5) #NUMBER OF EPOCHS\n",
    "\n",
    "torch.manual_seed(10)\n",
    "imIdx = torch.randperm(1).tolist()\n",
    "\n",
    "## Create dataset\n",
    "dataSetTrain = DNADataset(imDr, get_transform(train = True), labels, imDx=imIdx)\n",
    "dataSetTest = DNADataset(imDr, get_transform(train = False), labels, imDx=imIdx)\n",
    "\n",
    "data_loader_train=[]\n",
    "data_loader_test=[]\n",
    "\n",
    "# ## Create dataloaders\n",
    "# Get subset\n",
    "torch.manual_seed(10)\n",
    "indices = torch.randperm(len(dataSetTrain)).tolist()\n",
    "\n",
    "noTrain = int(len(dataSetTrain)*0.7)\n",
    "train=[]\n",
    "test=[]\n",
    "for epoch in epoch:\n",
    "    indices = torch.randperm(len(dataSetTrain)).tolist()\n",
    "    TotalSet=list(range(len(dataSetTrain)))\n",
    "    Kfifth = len(dataSetTrain)//k\n",
    "    split= epoch%k\n",
    "    train.append(TotalSet[(split+1)*Kfifth:]+TotalSet[:Kfifth*split])\n",
    "\n",
    "    indices = torch.randperm(len(dataSetTest)).tolist()\n",
    "    TotalSet=list(range(len(dataSetTrain)))\n",
    "    test.append(TotalSet[split*Kfifth:(split+1)*Kfifth])\n",
    "\n",
    "#dataset_train = torch.utils.data.Subset(dataSetTrain, indices[-noTrain:])\n",
    "\n",
    "#dataset_test = torch.utils.data.Subset(dataSetTest, indices[:-noTrain])\n",
    "#len(indices), len(indices[:-50]), len(indices[-50:]), 50/191, type(dataset_test)\n",
    "\n",
    "dataset_sizes = {'train': len(train[0]), 'val': len(test[0])}\n",
    "\n",
    "#dataset_train = torch.utils.data.Subset(dataSetTrain, train)\n",
    "#data_loader_train.append(dataset_train)\n",
    "#dataset_testo = torch.utils.data.Subset(dataSetTest, test)\n",
    "#data_loader_test.append(dataset_testo)\n",
    "# define training and validation data loaders\n",
    "\n",
    "\n",
    "# Collate function (gathers together the outputs)\n",
    "# def collate_fn(batch):\n",
    "#     return tuple(zip(*batch))\n",
    "\n",
    "#len(indices[-noTrain:]), dataset_sizes\n",
    "#dataset_test[0][1], dataset_test[3][1], dataset_test[-1][1], dataSetTrain.imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights= [0.924865,0.075135] #1-(#inclass/ #intotal )\n",
    "class_weights = torch.Tensor(class_weights)\n",
    "class_weights=class_weights.to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight = class_weights) \n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model.parameters(), weight_decay=1e-2) # standard ADAM optimiser\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n",
      "val Loss: 0.0616 Acc: 0.7740\n",
      "proper accuracy=\n",
      "tensor(0.9765, device='cuda:0')\n",
      "damaged accuracy=\n",
      "tensor(0.9766, device='cuda:0')\n",
      "undamaged accuracy=\n",
      "tensor(0.5714, device='cuda:0')\n",
      "[[25713   616]\n",
      " [    3     4]]\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "val Loss: 0.1522 Acc: 0.9152\n",
      "proper accuracy=\n",
      "tensor(0.9312, device='cuda:0')\n",
      "damaged accuracy=\n",
      "tensor(0.9316, device='cuda:0')\n",
      "undamaged accuracy=\n",
      "tensor(0.8988, device='cuda:0')\n",
      "[[24294  1785]\n",
      " [   26   231]]\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "val Loss: 0.2479 Acc: 0.8780\n",
      "proper accuracy=\n",
      "tensor(0.8441, device='cuda:0')\n",
      "damaged accuracy=\n",
      "tensor(0.8397, device='cuda:0')\n",
      "undamaged accuracy=\n",
      "tensor(0.9163, device='cuda:0')\n",
      "[[20840  3979]\n",
      " [  127  1390]]\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "val Loss: 0.2130 Acc: 0.8468\n",
      "proper accuracy=\n",
      "tensor(0.7885, device='cuda:0')\n",
      "damaged accuracy=\n",
      "tensor(0.7087, device='cuda:0')\n",
      "undamaged accuracy=\n",
      "tensor(0.9849, device='cuda:0')\n",
      "[[13275  5456]\n",
      " [  115  7490]]\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "val Loss: 0.1534 Acc: 0.9139\n",
      "proper accuracy=\n",
      "tensor(0.9541, device='cuda:0')\n",
      "damaged accuracy=\n",
      "tensor(0.9557, device='cuda:0')\n",
      "undamaged accuracy=\n",
      "tensor(0.8720, device='cuda:0')\n",
      "[[24685  1143]\n",
      " [   65   443]]\n",
      "\n",
      "Training complete in 2m 29s\n",
      "Best val Acc: 0.915193\n"
     ]
    }
   ],
   "source": [
    "model_ft, con_mat = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, \n",
    "                                    train,test,num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cf_matrix = confusion_matrix_calc(data_loader_test, labels, model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "con_mat=con_mat.cpu().numpy()\n",
    "df_cm_ratio = pd.DataFrame(con_mat / np.sum(con_mat, axis=1)[:, None], index=[i for i in labels],\n",
    "                     columns=[i for i in labels])\n",
    "plt.figure(figsize=(12, 7))\n",
    "sn.heatmap(df_cm_ratio, annot=True)\n",
    "# plt.save(\"Confusion_matrix_ratio.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_cm_raw = pd.DataFrame(con_mat, index=[i for i in labels],\n",
    "                     columns=[i for i in labels])\n",
    "plt.figure(figsize=(12, 7))\n",
    "sn.heatmap(df_cm_raw, annot=True)\n",
    "# plt.save(\"Confusion_matrix_raw.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24294,  1785],\n",
       "       [   26,   231]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
